import pandas as pd
import numpy as np
import warnings
from sklearn.exceptions import ConvergenceWarning
import subprocess
import os
import multiprocessing
import json
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# CUDA í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (GPU ì‚¬ìš© ê°•ì œ)
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['XGBOOST_USE_CUDA'] = '1'
os.environ['LIGHTGBM_USE_GPU'] = '1'

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report,
    top_k_accuracy_score, balanced_accuracy_score, roc_auc_score
)
from sklearn.preprocessing import label_binarize

from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.combine import SMOTEENN

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

warnings.filterwarnings("ignore")

# íŠ¹ì • ê²½ê³ ë§Œ ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.preprocessing._encoders")
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.feature_selection._univariate_selection")
warnings.filterwarnings("ignore", category=RuntimeWarning, module="sklearn.feature_selection._univariate_selection")
warnings.filterwarnings("ignore", category=ConvergenceWarning, module="sklearn.linear_model._sag")

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
    if result.returncode == 0:
        print("âœ… GPU ì‚¬ìš© ê°€ëŠ¥: NVIDIA GPUê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸš€ GPU ê°€ì†ì´ í™œì„±í™”ë˜ì–´ í•™ìŠµ ì†ë„ê°€ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.")
        
        # CUDA ë²„ì „ í™•ì¸
        try:
            cuda_result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
            if cuda_result.returncode == 0:
                cuda_version = cuda_result.stdout.split('release ')[1].split(',')[0]
                print(f"ğŸ”§ CUDA ë²„ì „: {cuda_version}")
        except:
            print("âš ï¸ CUDA ë²„ì „ í™•ì¸ ì‹¤íŒ¨")
        
        # GPU ìƒì„¸ ì •ë³´ ì¶œë ¥
        gpu_info = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'], 
                                capture_output=True, text=True)
        if gpu_info.returncode == 0:
            print(f"ğŸ“Š GPU ì •ë³´: {gpu_info.stdout.strip()}")
        
        # ì´ˆê¸° GPU ì‚¬ìš©ë¥  í™•ì¸
        gpu_util = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                capture_output=True, text=True)
        if gpu_util.returncode == 0:
            print(f"ğŸ–¥ï¸ ì´ˆê¸° GPU ì‚¬ìš©ë¥ : {gpu_util.stdout.strip()}%")
            
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        print(f"ğŸ”§ CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}")
        print(f"ğŸ”§ XGBOOST_USE_CUDA: {os.environ.get('XGBOOST_USE_CUDA', 'Not set')}")
        print(f"ğŸ”§ LIGHTGBM_USE_GPU: {os.environ.get('LIGHTGBM_USE_GPU', 'Not set')}")
            
    else:
        print("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€: NVIDIA GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
except:
    print("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€: nvidia-smi ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

print()

# CPU ì½”ì–´ ìˆ˜ í™•ì¸
cpu_count = multiprocessing.cpu_count()
print(f"ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:")
print(f"  - CPU ì½”ì–´ ìˆ˜: {cpu_count}ê°œ")
print(f"  - GPU ì‚¬ìš©: í™œì„±í™”")
print()

# --------------------------------------------------
# 1) ë°ì´í„° ì ì¬ ë° ê°€ê³µ
# --------------------------------------------------
df = pd.read_csv("new_merged_data/df_result2_mapping1.csv", dtype=str)
age_cols = ["59ì´í•˜", "60-64", "65-69", "70-79", "80-89", "90ì´ìƒ"]

m = df.melt(
    id_vars=["ë…„ë„", "êµ¬ë¶„", "ì§€ì—­", "ìƒë³‘ì½”ë“œ", "ì§„ë£Œê³¼"],
    value_vars=age_cols,
    var_name="age_group",
    value_name="count",
)
m["count"] = pd.to_numeric(m["count"], errors="coerce").fillna(0).astype(int)
m["ëŒ€í‘œì§„ë£Œê³¼"] = m["ì§„ë£Œê³¼"]
train = m[m["ëŒ€í‘œì§„ë£Œê³¼"].notna()]

# ê°•í™”ëœ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
train["year_num"] = train["ë…„ë„"].astype(int) - train["ë…„ë„"].astype(int).min()

# ì—°ë ¹ëŒ€ ìˆ˜ì¹˜í™” (ì¤‘ê°„ê°’ ì‚¬ìš©)
age_mapping = {
    "59ì´í•˜": 30, "60-64": 62, "65-69": 67, 
    "70-79": 75, "80-89": 85, "90ì´ìƒ": 95
}
train["age_num"] = train["age_group"].map(age_mapping)

# ì§€ì—­ë³„ íŠ¹ì„± (ëŒ€ë„ì‹œ vs ì¤‘ì†Œë„ì‹œ)
major_cities = ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „"]
train["is_major_city"] = train["ì§€ì—­"].isin(major_cities).astype(int)

# êµ¬ë¶„ë³„ íŠ¹ì„± (ì…ì› vs ì™¸ë˜)
train["is_inpatient"] = (train["êµ¬ë¶„"] == "ì…ì›").astype(int)

# ìƒë³‘ì½”ë“œ ê¸°ë°˜ í”¼ì²˜ (ì²« 3ìë¦¬ë¡œ ê·¸ë£¹í™”)
train["disease_group"] = train["ìƒë³‘ì½”ë“œ"].str[:3]

# ì—°ë„ë³„ íŠ¸ë Œë“œ
train["year_trend"] = train["year_num"] ** 2

# ë³µí•© í”¼ì²˜
train["age_city_interaction"] = train["age_num"] * train["is_major_city"]
train["age_year_interaction"] = train["age_num"] * train["year_num"]

# ì§€ì—­-ì—°ë ¹ëŒ€ ì¡°í•©
train["region_age"] = train["ì§€ì—­"] + "_" + train["age_group"]

X = train[["year_num", "age_num", "is_major_city", "is_inpatient", 
           "year_trend", "age_city_interaction", "age_year_interaction", 
           "ì§€ì—­", "age_group", "êµ¬ë¶„", "disease_group", "region_age"]]
y = train["ëŒ€í‘œì§„ë£Œê³¼"]
w = train["count"]

# --------------------------------------------------
# 2) í•™ìŠµ / ê²€ì¦ ë¶„ë¦¬
# --------------------------------------------------
X_tr, X_te, y_tr, y_te, w_tr, w_te = train_test_split(
    X, y, w, test_size=0.20, stratify=y, random_state=42
)

# --------------------------------------------------
# 3) ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# --------------------------------------------------
# ìˆ˜ì¹˜í˜• í”¼ì²˜
num_cols = ["year_num", "age_num", "is_major_city", "is_inpatient", 
            "year_trend", "age_city_interaction", "age_year_interaction"]

# ë²”ì£¼í˜• í”¼ì²˜
cat_cols = ["ì§€ì—­", "age_group", "êµ¬ë¶„", "disease_group", "region_age"]

preprocessor = ColumnTransformer(
    [
        ("ohe", OneHotEncoder(drop="first", sparse_output=False, handle_unknown="ignore"), cat_cols),
        ("scale", StandardScaler(), num_cols),
    ]
)

# --------------------------------------------------
# 4) XGBìš© ë˜í¼ í´ë˜ìŠ¤
# --------------------------------------------------
class XGBWrapper(XGBClassifier):
    """ë¬¸ìâ†’ìˆ«ì ë¼ë²¨ì„ ë‚´ë¶€ ë³€í™˜í•˜ê³ , ì›ë³¸ ë¼ë²¨ì€ orig_classes_ì— ì €ì¥"""
    def fit(self, X, y, **kwargs):
        self._le = LabelEncoder()
        y_enc = self._le.fit_transform(y)
        super().fit(X, y_enc, **kwargs)
        self.orig_classes_ = self._le.classes_
        return self

    def predict(self, X):
        return self._le.inverse_transform(super().predict(X))

    def predict_proba(self, X):
        return super().predict_proba(X)

# --------------------------------------------------
# 5) íŒŒì´í”„ë¼ì¸ & ê·¸ë¦¬ë“œ ì •ì˜ í•¨ìˆ˜
# --------------------------------------------------
def make_pipeline(clf, param_grid):
    pipe = ImbPipeline(
        [
            ("prep", preprocessor),
            ("smote", SMOTE(random_state=42)),
            ("variance", VarianceThreshold(threshold=0.01)),  # ìƒìˆ˜ í”¼ì²˜ ì œê±°
            ("select", SelectKBest(f_classif)),
            ("clf", clf),
        ]
    )
    return pipe, param_grid

# k ê°’ì„ ë™ì ìœ¼ë¡œ ì„¤ì •
# preprocessorë¥¼ ë¨¼ì € fitì‹œì¼œì•¼ í•¨
preprocessor.fit(X_tr)
n_features_after_prep = len(preprocessor.get_feature_names_out())  # ì „ì²˜ë¦¬ í›„ í”¼ì²˜ ìˆ˜
max_k = min(n_features_after_prep, 100)  # ìµœëŒ€ 100ê°œë¡œ í™•ì¥

# Logistic Regression
pipe_lr, params_lr = make_pipeline(
    LogisticRegression(
        penalty="l1", solver="saga", max_iter=5000, class_weight="balanced"
    ),
    {
        "select__k": [max_k//4, max_k//2, max_k],
        "clf__C": [0.001, 0.01, 0.1, 1, 10],
    },
)

# Random Forest
pipe_rf, params_rf = make_pipeline(
    RandomForestClassifier(class_weight="balanced", random_state=42),
    {
        "select__k": [max_k//4, max_k//2, max_k],
        "clf__n_estimators": [100, 200, 300],
        "clf__max_depth": [None, 10, 20, 30],
        "clf__min_samples_split": [2, 5, 10],
    },
)

# XGBoost
pipe_xgb, params_xgb = make_pipeline(
    XGBWrapper(
        eval_metric="mlogloss",
        random_state=42,
        tree_method="hist",  # hist ì‚¬ìš©
        device="cuda",  # ìƒˆë¡œìš´ GPU ì„¤ì • ë°©ì‹
        max_bin=256,  # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        single_precision_histogram=True,  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
        enable_categorical=False,  # ì¹´í…Œê³ ë¦¬í˜• ë¹„í™œì„±í™”
        max_leaves=0,  # GPU ìµœì í™”
        grow_policy="lossguide",  # GPU ìµœì í™”
    ),
    {
        "select__k": [max_k//4, max_k//2, max_k],
        "clf__n_estimators": [200, 400, 600],
        "clf__max_depth": [3, 6, 9],
        "clf__learning_rate": [0.01, 0.1, 0.2],
        "clf__reg_alpha": [0, 0.1, 1],
        "clf__reg_lambda": [0, 0.1, 1],
    },
)

# LightGBM
pipe_lgb, params_lgb = make_pipeline(
    LGBMClassifier(
        objective="multiclass",
        random_state=42,
        class_weight="balanced",
        verbose=-1,
        device="gpu",  # GPU ì‚¬ìš©
        gpu_platform_id=0,  # GPU í”Œë«í¼ ID
        gpu_device_id=0,  # GPU ë””ë°”ì´ìŠ¤ ID
        force_col_wise=True,  # GPU ìµœì í™”
        gpu_use_dp=False,  # ë‹¨ì •ë°€ë„ ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        max_bin=255,  # GPU ìµœì í™”
        num_leaves=31,  # ê³ ì •ê°’ìœ¼ë¡œ ê²½ê³  ì œê±°
        min_child_samples=20,  # ê³ ì •ê°’ìœ¼ë¡œ ê²½ê³  ì œê±°
        subsample=1.0,  # ê³ ì •ê°’ìœ¼ë¡œ ê²½ê³  ì œê±°
        colsample_bytree=1.0,  # ê³ ì •ê°’ìœ¼ë¡œ ê²½ê³  ì œê±°
        # ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •
        n_jobs=1,  # GPU ì‚¬ìš©ì‹œ ë‹¨ì¼ ìŠ¤ë ˆë“œ
        deterministic=True,  # ì¬í˜„ì„± ë³´ì¥
        force_row_wise=False,  # GPU ìµœì í™”
        # GPU ê°•ì œ ì‚¬ìš©ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •
        gpu_use_dp_for_histogram=False,  # íˆìŠ¤í† ê·¸ë¨ë„ ë‹¨ì •ë°€ë„
        gpu_use_dp_for_histogram_bin=False,  # íˆìŠ¤í† ê·¸ë¨ ë¹ˆë„ë„ ë‹¨ì •ë°€ë„
        gpu_use_dp_for_histogram_bin_leaf=False,  # ë¦¬í”„ë³„ íˆìŠ¤í† ê·¸ë¨ë„ ë‹¨ì •ë°€ë„
        gpu_use_dp_for_histogram_bin_leaf_grad=False,  # ê·¸ë˜ë””ì–¸íŠ¸ë„ ë‹¨ì •ë°€ë„
        gpu_use_dp_for_histogram_bin_leaf_hess=False,  # í—¤ì‹œì•ˆë„ ë‹¨ì •ë°€ë„
        gpu_use_dp_for_histogram_bin_leaf_hess_grad=False,  # í—¤ì‹œì•ˆ ê·¸ë˜ë””ì–¸íŠ¸ë„ ë‹¨ì •ë°€ë„
        gpu_use_dp_for_histogram_bin_leaf_hess_grad_hess=False,  # í—¤ì‹œì•ˆ ê·¸ë˜ë””ì–¸íŠ¸ í—¤ì‹œì•ˆë„ ë‹¨ì •ë°€ë„
    ),
    {
        "select__k": [max_k//4, max_k//2, max_k],
        "clf__n_estimators": [200, 400, 600],
        "clf__max_depth": [3, 6, 9],
        "clf__learning_rate": [0.01, 0.1, 0.2],
        "clf__num_leaves": [31, 63, 127],
    },
)

# Gradient Boosting
pipe_gb, params_gb = make_pipeline(
    GradientBoostingClassifier(random_state=42),
    {
        "select__k": [max_k//4, max_k//2, max_k],
        "clf__n_estimators": [100, 200, 300],
        "clf__max_depth": [3, 6, 9],
        "clf__learning_rate": [0.01, 0.1, 0.2],
    },
)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# --------------------------------------------------
# 6) ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
# --------------------------------------------------
print("=== ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
print(f"ì „ì²˜ë¦¬ í›„ í”¼ì²˜ ìˆ˜: {n_features_after_prep}")
print(f"ìµœëŒ€ k ê°’: {max_k}")
print(f"í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_tr.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_te.shape}")
print(f"í´ë˜ìŠ¤ ìˆ˜: {len(np.unique(y_tr))}")
print(f"í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
for cls, count in y_tr.value_counts().items():
    print(f"  {cls}: {count}ê°œ")
print()

grids = {}
for i, (name, (pipe, params)) in enumerate(zip(
    ["lr", "rf", "xgb", "lgb", "gb"],
    [
        (pipe_lr, params_lr),
        (pipe_rf, params_rf),
        (pipe_xgb, params_xgb),
        (pipe_lgb, params_lgb),
        (pipe_gb, params_gb),
    ],
), 1):
    print(f"=== ëª¨ë¸ {i}/5: {name.upper()} ëª¨ë¸ í•™ìŠµ ì¤‘... ===")
    print(f"íŒŒë¼ë¯¸í„° ì¡°í•© ìˆ˜: {len([(k, v) for k, v in params.items() for v in v])}")
    print(f"ì˜ˆìƒ í•™ìŠµ ì‹œê°„: ì•½ 1-3ë¶„")
    
    # GPU ì‚¬ìš© í™•ì¸ (XGBoost, LightGBMì˜ ê²½ìš°)
    if name in ['xgb', 'lgb']:
        print(f"ğŸ” {name.upper()} GPU ì‚¬ìš© í™•ì¸ ì¤‘...")
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            print(f"nvidia-smi ëª…ë ¹ì–´ ê²°ê³¼: {result.returncode}")
            print(f"nvidia-smi ì¶œë ¥: {result.stdout.strip()}")
            if result.returncode == 0:
                gpu_util = result.stdout.strip()
                print(f"ğŸ–¥ï¸ í•™ìŠµ ì „ GPU ì‚¬ìš©ë¥ : {gpu_util}%")
            else:
                print(f"âŒ nvidia-smi ì˜¤ë¥˜: {result.stderr}")
        except Exception as e:
            print(f"âŒ GPU í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ í™•ì¸
        try:
            mem_result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True)
            if mem_result.returncode == 0:
                mem_info = mem_result.stdout.strip().split(',')
                if len(mem_info) >= 2:
                    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {mem_info[0]}/{mem_info[1]} MB")
        except:
            pass
    
    # CPU ëª¨ë¸ë“¤: ë©€í‹°ì½”ì–´ í™œìš©
    n_jobs = max(1, int(cpu_count * 0.75))  # 75% ì½”ì–´ í™œìš©
    
    grid = GridSearchCV(
        pipe, params, cv=cv, scoring="accuracy", n_jobs=n_jobs, verbose=1
    )
    print(f"GridSearchCV ì‹œì‘...")
    grid.fit(X_tr, y_tr)  # sample_weight ë¯¸ì‚¬ìš©
    grids[name] = grid
    
    # í•™ìŠµ í›„ GPU ì‚¬ìš©ë¥  í™•ì¸
    if name in ['xgb', 'lgb']:
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                gpu_util = result.stdout.strip()
                print(f"ğŸ–¥ï¸ í•™ìŠµ í›„ GPU ì‚¬ìš©ë¥ : {gpu_util}%")
        except:
            pass
    
    print(f"âœ… {name.upper()} ìµœì  íŒŒë¼ë¯¸í„°: {grid.best_params_}")
    print(f"âœ… {name.upper()} ìµœì  ì ìˆ˜: {grid.best_score_:.4f}")
    print(f"âœ… {name.upper()} í•™ìŠµ ì™„ë£Œ ({i}/5)")
    print()

# --------------------------------------------------
# 7) ì•™ìƒë¸” (Voting & Stacking)
# --------------------------------------------------
print("=== ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì¤‘... ===")

# estimators ì •ì˜
estimators = [(n, grids[n].best_estimator_) for n in ["lr", "rf", "xgb", "lgb", "gb"]]
print(f"ì•™ìƒë¸”ì— ì‚¬ìš©í•  ëª¨ë¸: {[name for name, _ in estimators]}")

print("1/2: Voting Classifier í•™ìŠµ ì¤‘...")
vot = VotingClassifier(estimators=estimators, voting="soft")
vot.fit(X_tr, y_tr)
print("âœ… Voting Classifier ì™„ë£Œ")

print("2/2: Stacking Classifier í•™ìŠµ ì¤‘...")
print("  - ë©”íƒ€ ëª¨ë¸: LogisticRegression")
print("  - êµì°¨ê²€ì¦: 5-fold")
print("  - ë³‘ë ¬ ì²˜ë¦¬: CPU ëª¨ë¸ê³¼ GPU ëª¨ë¸ í˜¼ì¬ë¡œ ì¸í•´ ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‚¬ìš©")

# Stacking: ë³‘ë ¬ ì²˜ë¦¬
n_jobs = max(1, int(cpu_count * 0.5))   # 50% ì½”ì–´ í™œìš©
stack = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(max_iter=5000),
    cv=cv,
    n_jobs=n_jobs,
)
stack.fit(X_tr, y_tr)
print("âœ… Stacking Classifier ì™„ë£Œ")
print()

# --------------------------------------------------
# 8) í‰ê°€ í•¨ìˆ˜
# --------------------------------------------------
def eval_model(name, model, X, y_true, w):
    print(f"=== {name} ëª¨ë¸ í‰ê°€ ì¤‘... ===")
    y_pred = model.predict(X)
    proba = model.predict_proba(X) if hasattr(model, "predict_proba") else None

    print(f"\n=== {name} ===")
    acc = accuracy_score(y_true, y_pred, sample_weight=w)
    macro_f1 = f1_score(y_true, y_pred, average="macro", sample_weight=w)
    print(f"Accuracy: {acc:.4f}")
    print(f"Macro-F1: {macro_f1:.4f}")

    if proba is not None:
        # ---- class_order ì•ˆì „ ì¶”ì¶œ ----
        class_order = getattr(model, "orig_classes_", None)
        if class_order is None:
            class_order = getattr(model, "classes_", None)
        if class_order is None:
            class_order = np.unique(y_true)
        # --------------------------------

        top3_acc = top_k_accuracy_score(y_true, proba, k=3, sample_weight=w)
        bal_acc = balanced_accuracy_score(y_true, y_pred, sample_weight=w)
        print(f"Top-3 Accuracy: {top3_acc:.4f}")
        print(f"Balanced Accuracy: {bal_acc:.4f}")

        y_bin = label_binarize(y_true, classes=class_order)
        roc_auc = roc_auc_score(y_bin, proba, average="macro", sample_weight=w)
        print(f"Macro ROC-AUC: {roc_auc:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, sample_weight=w, digits=3))

print("=== ê¸°ë³¸ ëª¨ë¸ í‰ê°€ ì‹œì‘ ===")
print("í‰ê°€í•  ëª¨ë¸: XGB, LGB, GB, Voting, Stacking")
print()

for i, (nm, mdl) in enumerate([
    ("XGB", grids["xgb"].best_estimator_),
    ("LGB", grids["lgb"].best_estimator_),
    ("GB", grids["gb"].best_estimator_),
    ("Voting", vot),
    ("Stacking", stack),
], 1):
    print(f"--- ëª¨ë¸ {i}/5: {nm} ---")
    eval_model(nm, mdl, X_te, y_te, w_te)
    print()

print("\n" + "="*60)
print("=== ê°œì„ ëœ ìƒ˜í”Œë§ ê¸°ë²• í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
print("="*60)

# 1. ê·¹ë‹¨ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
print("1/3: ê·¹ë‹¨ì  ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
# ë¬¸ìì—´ ë¼ë²¨ì„ ì •ìˆ˜ë¡œ ë³€í™˜
le_weights = LabelEncoder()
y_tr_encoded = le_weights.fit_transform(y_tr)
class_counts = np.bincount(y_tr_encoded)
total_samples = len(y_tr_encoded)
extreme_weights = (total_samples / (len(class_counts) * class_counts)) ** 1.5
print(f"ê°€ì¤‘ì¹˜ ë²”ìœ„: {extreme_weights.min():.2f} ~ {extreme_weights.max():.2f}")
print("âœ… ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ")
print()

# 2. ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ê¸°ë²• ì‹œë„
sampling_methods = {
    'adasyn': ADASYN(random_state=42),
    'borderline_smote': BorderlineSMOTE(random_state=42),
    'smote_enn': SMOTEENN(random_state=42)
}

print(f"2/3: {len(sampling_methods)}ê°€ì§€ ìƒ˜í”Œë§ ê¸°ë²• í…ŒìŠ¤íŠ¸")
print(f"í…ŒìŠ¤íŠ¸í•  ê¸°ë²•: {list(sampling_methods.keys())}")
print()

# 3. ê° ë°©ë²•ë³„ë¡œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
for i, (name, sampler) in enumerate(sampling_methods.items(), 1):
    print(f"--- ìƒ˜í”Œë§ ê¸°ë²• {i}/{len(sampling_methods)}: {name.upper()} ---")
    print(f"ìƒ˜í”ŒëŸ¬: {type(sampler).__name__}")
    print(f"ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: ì•½ 30ì´ˆ-1ë¶„")
    
    try:
        # ì „ì²˜ë¦¬ ì ìš©
        print("ì „ì²˜ë¦¬ ì ìš© ì¤‘...")
        X_tr_preprocessed = preprocessor.transform(X_tr)
        print(f"ì „ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {X_tr_preprocessed.shape}")
        
        # ìƒ˜í”Œë§ ì ìš© (ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
        print("ìƒ˜í”Œë§ ì ìš© ì¤‘...")
        X_resampled, y_resampled = sampler.fit_resample(X_tr_preprocessed, y_tr)
        print(f"ìƒ˜í”Œë§ í›„ ë°ì´í„° í¬ê¸°: {X_resampled.shape}")
        print(f"ìƒ˜í”Œë§ í›„ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
        for cls, count in pd.Series(y_resampled).value_counts().items():
            print(f"  {cls}: {count}ê°œ")
        
        # ìƒ˜í”Œë§ í›„ ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ê³„ì‚°
        print("ìƒ˜í”Œë§ í›„ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
        le_resampled = LabelEncoder()
        y_resampled_encoded = le_resampled.fit_transform(y_resampled)
        class_counts_resampled = np.bincount(y_resampled_encoded)
        total_samples_resampled = len(y_resampled_encoded)
        resampled_weights = (total_samples_resampled / (len(class_counts_resampled) * class_counts_resampled)) ** 1.5
        sample_weights_resampled = np.array([resampled_weights[label] for label in y_resampled_encoded])
        print(f"ìƒ˜í”Œë§ í›„ ê°€ì¤‘ì¹˜ ë²”ìœ„: {resampled_weights.min():.2f} ~ {resampled_weights.max():.2f}")
        
        # XGBWrapper ì‚¬ìš©í•˜ì—¬ ë¬¸ìì—´ ë¼ë²¨ ì²˜ë¦¬
        pipe = Pipeline([
            ('clf', XGBWrapper(
                eval_metric="mlogloss",
                random_state=42,
                tree_method="hist",  # hist ì‚¬ìš©
                device="cuda",  # ìƒˆë¡œìš´ GPU ì„¤ì • ë°©ì‹
                max_bin=256,  # GPU ë©”ëª¨ë¦¬ ìµœì í™”
                single_precision_histogram=True,  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
                enable_categorical=False,  # ì¹´í…Œê³ ë¦¬í˜• ë¹„í™œì„±í™”
                max_leaves=0,  # GPU ìµœì í™”
                grow_policy="lossguide",  # GPU ìµœì í™”
            ))
        ])
        
        print("ëª¨ë¸ í•™ìŠµ ì¤‘...")
        # GPU ì‚¬ìš©ë¥  í™•ì¸
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                gpu_util = result.stdout.strip()
                print(f"ğŸ–¥ï¸ ìƒ˜í”Œë§ í•™ìŠµ ì „ GPU ì‚¬ìš©ë¥ : {gpu_util}%")
        except:
            pass
            
        pipe.fit(X_resampled, y_resampled, clf__sample_weight=sample_weights_resampled)
        
        # í•™ìŠµ í›„ GPU ì‚¬ìš©ë¥  í™•ì¸
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                gpu_util = result.stdout.strip()
                print(f"ğŸ–¥ï¸ ìƒ˜í”Œë§ í•™ìŠµ í›„ GPU ì‚¬ìš©ë¥ : {gpu_util}%")
        except:
            pass
            
        print(f"âœ… {name.upper()} í•™ìŠµ ì™„ë£Œ")

        # í‰ê°€ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ì „ì²˜ë¦¬)
        print(f"{name.upper()} í‰ê°€ ì¤‘...")
        X_te_preprocessed = preprocessor.transform(X_te)
        y_pred = pipe.predict(X_te_preprocessed)
        eval_model(f"{name.upper()}", pipe, X_te_preprocessed, y_te, w_te)
        
    except Exception as e:
        print(f"âŒ {name.upper()} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("ë‹¤ìŒ ìƒ˜í”Œë§ ê¸°ë²•ìœ¼ë¡œ ì§„í–‰...")
    
    print()

print("3/3: ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ")
print("="*60)
print("ğŸ‰ ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print("="*60)

# --------------------------------------------------
# 9) ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
# --------------------------------------------------
print("\n" + "="*60)
print("=== ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥ ì‹œì‘ ===")
print("="*60)

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
results_dir = "model_results"
os.makedirs(results_dir, exist_ok=True)
os.makedirs(f"{results_dir}/predictions", exist_ok=True)
os.makedirs(f"{results_dir}/performance", exist_ok=True)
os.makedirs(f"{results_dir}/features", exist_ok=True)

print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {results_dir}/")

# --------------------------------------------------
# 1) ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥
# --------------------------------------------------
print("1/6: ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥ ì¤‘...")

def extract_model_performance(model_name, model, X, y_true, w):
    """ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ"""
    y_pred = model.predict(X)
    proba = model.predict_proba(X) if hasattr(model, "predict_proba") else None
    
    # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ
    acc = accuracy_score(y_true, y_pred, sample_weight=w)
    macro_f1 = f1_score(y_true, y_pred, average="macro", sample_weight=w)
    weighted_f1 = f1_score(y_true, y_pred, average="weighted", sample_weight=w)
    bal_acc = balanced_accuracy_score(y_true, y_pred, sample_weight=w)
    
    # í´ë˜ìŠ¤ë³„ F1-score
    f1_scores = f1_score(y_true, y_pred, average=None, sample_weight=w)
    
    # Top-k ì •í™•ë„
    top3_acc = top_k_accuracy_score(y_true, proba, k=3, sample_weight=w) if proba is not None else None
    top5_acc = top_k_accuracy_score(y_true, proba, k=5, sample_weight=w) if proba is not None else None
    
    # ROC-AUC
    if proba is not None:
        class_order = getattr(model, "orig_classes_", None)
        if class_order is None:
            class_order = getattr(model, "classes_", None)
        if class_order is None:
            class_order = np.unique(y_true)
        
        y_bin = label_binarize(y_true, classes=class_order)
        roc_auc_macro = roc_auc_score(y_bin, proba, average="macro", sample_weight=w)
        roc_auc_weighted = roc_auc_score(y_bin, proba, average="weighted", sample_weight=w)
    else:
        roc_auc_macro = None
        roc_auc_weighted = None
    
    return {
        "model_name": model_name,
        "accuracy": acc,
        "macro_f1": macro_f1,
        "weighted_f1": weighted_f1,
        "balanced_accuracy": bal_acc,
        "top3_accuracy": top3_acc,
        "top5_accuracy": top5_acc,
        "roc_auc_macro": roc_auc_macro,
        "roc_auc_weighted": roc_auc_weighted,
        "f1_scores_by_class": f1_scores.tolist() if f1_scores is not None else None,
        "classes": class_order.tolist() if 'class_order' in locals() else None
    }

# ê¸°ë³¸ ëª¨ë¸ë“¤ ì„±ëŠ¥ ì¶”ì¶œ
basic_models = {
    "XGB": grids["xgb"].best_estimator_,
    "LGB": grids["lgb"].best_estimator_,
    "GB": grids["gb"].best_estimator_,
    "RF": grids["rf"].best_estimator_,
    "LR": grids["lr"].best_estimator_,
    "Voting": vot,
    "Stacking": stack
}

performance_results = []
for name, model in basic_models.items():
    print(f"  - {name} ëª¨ë¸ ì„±ëŠ¥ ì¶”ì¶œ ì¤‘...")
    perf = extract_model_performance(name, model, X_te, y_te, w_te)
    performance_results.append(perf)

# ìƒ˜í”Œë§ ëª¨ë¸ë“¤ ì„±ëŠ¥ ì¶”ì¶œ
sampling_results = []
for name, sampler in sampling_methods.items():
    try:
        # ìƒ˜í”Œë§ ëª¨ë¸ ì¬í•™ìŠµ (ê°„ë‹¨í•œ ë²„ì „)
        X_tr_preprocessed = preprocessor.transform(X_tr)
        X_resampled, y_resampled = sampler.fit_resample(X_tr_preprocessed, y_tr)
        
        # ê°„ë‹¨í•œ XGBoost ëª¨ë¸ë¡œ ì„±ëŠ¥ ì¸¡ì •
        simple_model = XGBWrapper(
            eval_metric="mlogloss",
            random_state=42,
            tree_method="hist",
            device="cuda",
            n_estimators=100
        )
        simple_model.fit(X_resampled, y_resampled)
        
        print(f"  - {name.upper()} ìƒ˜í”Œë§ ëª¨ë¸ ì„±ëŠ¥ ì¶”ì¶œ ì¤‘...")
        perf = extract_model_performance(f"{name.upper()}_sampling", simple_model, X_te, y_te, w_te)
        sampling_results.append(perf)
    except Exception as e:
        print(f"  - {name.upper()} ìƒ˜í”Œë§ ëª¨ë¸ ì„±ëŠ¥ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

# ì„±ëŠ¥ ê²°ê³¼ ì €ì¥
performance_df = pd.DataFrame(performance_results + sampling_results)
performance_df.to_csv(f"{results_dir}/performance/model_performance_comparison.csv", index=False, encoding='utf-8-sig')

# JSON í˜•íƒœë¡œë„ ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)
performance_metadata = {
    "timestamp": datetime.now().isoformat(),
    "test_data_size": len(X_te),
    "train_data_size": len(X_tr),
    "num_classes": len(np.unique(y_te)),
    "class_distribution": y_te.value_counts().to_dict(),
    "models": performance_results + sampling_results
}

with open(f"{results_dir}/performance/performance_metadata.json", 'w', encoding='utf-8') as f:
    json.dump(performance_metadata, f, ensure_ascii=False, indent=2)

print(f"âœ… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_dir}/performance/")

# --------------------------------------------------
# 2) ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì €ì¥
# --------------------------------------------------
print("2/6: ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì €ì¥ ì¤‘...")

grid_search_results = {}
for name, grid in grids.items():
    print(f"  - {name.upper()} ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì¶”ì¶œ ì¤‘...")
    
    # ìµœì  íŒŒë¼ë¯¸í„°
    best_params = grid.best_params_
    
    # ëª¨ë“  íŒŒë¼ë¯¸í„° ì¡°í•©ì˜ ê²°ê³¼
    cv_results = grid.cv_results_
    
    # ê²°ê³¼ ìš”ì•½
    grid_summary = {
        "model_name": name.upper(),
        "best_score": grid.best_score_,
        "best_params": best_params,
        "best_estimator_type": type(grid.best_estimator_).__name__,
        "cv_splits": cv_results.get('split0_test_score', []).shape[0] if 'split0_test_score' in cv_results else 5,
        "total_combinations": len(cv_results['mean_test_score']),
        "mean_test_scores": cv_results['mean_test_score'].tolist(),
        "std_test_scores": cv_results['std_test_score'].tolist(),
        "rank_test_scores": cv_results['rank_test_score'].tolist(),
        "param_combinations": []
    }
    
    # ê° íŒŒë¼ë¯¸í„° ì¡°í•©ë³„ ê²°ê³¼
    for i in range(len(cv_results['mean_test_score'])):
        param_combo = {}
        for param_name in cv_results['params'][i].keys():
            param_combo[param_name] = cv_results['params'][i][param_name]
        
        param_combo.update({
            "mean_test_score": cv_results['mean_test_score'][i],
            "std_test_score": cv_results['std_test_score'][i],
            "rank_test_score": cv_results['rank_test_scores'][i]
        })
        grid_summary["param_combinations"].append(param_combo)
    
    grid_search_results[name] = grid_summary

# ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì €ì¥
with open(f"{results_dir}/performance/grid_search_results.json", 'w', encoding='utf-8') as f:
    json.dump(grid_search_results, f, ensure_ascii=False, indent=2)

# ìš”ì•½ í…Œì´ë¸” ìƒì„±
grid_summary_df = pd.DataFrame([
    {
        "model": name,
        "best_score": result["best_score"],
        "best_params": str(result["best_params"]),
        "total_combinations": result["total_combinations"]
    }
    for name, result in grid_search_results.items()
])
grid_summary_df.to_csv(f"{results_dir}/performance/grid_search_summary.csv", index=False, encoding='utf-8-sig')

print(f"âœ… ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_dir}/performance/")

# --------------------------------------------------
# 3) í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥
# --------------------------------------------------
print("3/6: í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥ ì¤‘...")

feature_importance_results = {}

# í”¼ì²˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
feature_names = preprocessor.get_feature_names_out()

for name, model in basic_models.items():
    print(f"  - {name} í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ ì¤‘...")
    
    # ëª¨ë¸ì—ì„œ í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ
    if hasattr(model, 'feature_importances_'):
        # íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸
        importances = model.feature_importances_
    elif hasattr(model, 'coef_'):
        # ì„ í˜• ëª¨ë¸
        importances = np.abs(model.coef_).mean(axis=0)
    else:
        # ì•™ìƒë¸” ëª¨ë¸ì˜ ê²½ìš° ê°œë³„ ëª¨ë¸ë“¤ì˜ ì¤‘ìš”ë„ í‰ê· 
        if hasattr(model, 'estimators_'):
            importances = np.mean([est.feature_importances_ for est in model.estimators_ if hasattr(est, 'feature_importances_')], axis=0)
        else:
            importances = None
    
    if importances is not None:
        # í”¼ì²˜ ì¤‘ìš”ë„ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        importance_df = pd.DataFrame({
            'feature_name': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        feature_importance_results[name] = {
            "model_name": name,
            "feature_importance": importance_df.to_dict('records'),
            "top_features": importance_df.head(20).to_dict('records')
        }
        
        # ê°œë³„ CSV íŒŒì¼ë¡œ ì €ì¥
        importance_df.to_csv(f"{results_dir}/features/{name}_feature_importance.csv", index=False, encoding='utf-8-sig')

# ì „ì²´ í”¼ì²˜ ì¤‘ìš”ë„ ìš”ì•½
if feature_importance_results:
    # ëª¨ë“  ëª¨ë¸ì˜ í”¼ì²˜ ì¤‘ìš”ë„ í‰ê· 
    all_importances = []
    for name, result in feature_importance_results.items():
        importance_df = pd.DataFrame(result["feature_importance"])
        importance_df['model'] = name
        all_importances.append(importance_df)
    
    if all_importances:
        combined_importance = pd.concat(all_importances, ignore_index=True)
        avg_importance = combined_importance.groupby('feature_name')['importance'].agg(['mean', 'std', 'count']).reset_index()
        avg_importance = avg_importance.sort_values('mean', ascending=False)
        avg_importance.to_csv(f"{results_dir}/features/average_feature_importance.csv", index=False, encoding='utf-8-sig')

# í”¼ì²˜ ì¤‘ìš”ë„ ë©”íƒ€ë°ì´í„° ì €ì¥
with open(f"{results_dir}/features/feature_importance_metadata.json", 'w', encoding='utf-8') as f:
    json.dump(feature_importance_results, f, ensure_ascii=False, indent=2)

print(f"âœ… í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥ ì™„ë£Œ: {results_dir}/features/")

# --------------------------------------------------
# 4) ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ ì €ì¥
# --------------------------------------------------
print("4/6: ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ ì €ì¥ ì¤‘...")

prediction_probability_results = {}

for name, model in basic_models.items():
    print(f"  - {name} ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ ì¶”ì¶œ ì¤‘...")
    
    try:
        # ì˜ˆì¸¡ í™•ë¥ 
        proba = model.predict_proba(X_te)
        
        # í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        class_names = getattr(model, "orig_classes_", None)
        if class_names is None:
            class_names = getattr(model, "classes_", None)
        if class_names is None:
            class_names = [f"class_{i}" for i in range(proba.shape[1])]
        
        # ì˜ˆì¸¡ í™•ë¥  ë°ì´í„°í”„ë ˆì„ ìƒì„±
        proba_df = pd.DataFrame(proba, columns=class_names)
        proba_df['actual_class'] = y_te.values
        proba_df['predicted_class'] = model.predict(X_te)
        proba_df['confidence'] = proba.max(axis=1)  # ìµœëŒ€ í™•ë¥ ê°’
        proba_df['model'] = name
        
        # Top-3 ì˜ˆì¸¡
        top3_indices = np.argsort(proba, axis=1)[:, -3:][:, ::-1]
        top3_classes = class_names[top3_indices]
        top3_probs = np.take_along_axis(proba, top3_indices, axis=1)
        
        proba_df['top1_class'] = top3_classes[:, 0]
        proba_df['top1_prob'] = top3_probs[:, 0]
        proba_df['top2_class'] = top3_classes[:, 1]
        proba_df['top2_prob'] = top3_probs[:, 1]
        proba_df['top3_class'] = top3_classes[:, 2]
        proba_df['top3_prob'] = top3_probs[:, 2]
        
        # ì‹ ë¢°ë„ êµ¬ê°„ë³„ ë¶„ì„
        confidence_bins = [0, 0.5, 0.7, 0.8, 0.9, 1.0]
        confidence_labels = ['0-0.5', '0.5-0.7', '0.7-0.8', '0.8-0.9', '0.9-1.0']
        proba_df['confidence_bin'] = pd.cut(proba_df['confidence'], bins=confidence_bins, labels=confidence_labels)
        
        # ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì •í™•ë„
        confidence_accuracy = proba_df.groupby('confidence_bin').apply(
            lambda x: (x['actual_class'] == x['predicted_class']).mean()
        ).reset_index()
        confidence_accuracy.columns = ['confidence_bin', 'accuracy']
        
        prediction_probability_results[name] = {
            "model_name": name,
            "class_names": class_names.tolist(),
            "confidence_accuracy": confidence_accuracy.to_dict('records'),
            "probability_stats": {
                "mean_confidence": proba_df['confidence'].mean(),
                "std_confidence": proba_df['confidence'].std(),
                "min_confidence": proba_df['confidence'].min(),
                "max_confidence": proba_df['confidence'].max()
            }
        }
        
        # ê°œë³„ CSV íŒŒì¼ë¡œ ì €ì¥
        proba_df.to_csv(f"{results_dir}/predictions/{name}_prediction_probabilities.csv", index=False, encoding='utf-8-sig')
        
    except Exception as e:
        print(f"  - {name} ì˜ˆì¸¡ í™•ë¥  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

# ì˜ˆì¸¡ í™•ë¥  ë©”íƒ€ë°ì´í„° ì €ì¥
with open(f"{results_dir}/predictions/prediction_probability_metadata.json", 'w', encoding='utf-8') as f:
    json.dump(prediction_probability_results, f, ensure_ascii=False, indent=2)

print(f"âœ… ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ ì €ì¥ ì™„ë£Œ: {results_dir}/predictions/")

# --------------------------------------------------
# 5) í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ì €ì¥
# --------------------------------------------------
print("5/6: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ì €ì¥ ì¤‘...")

class_performance_results = {}

for name, model in basic_models.items():
    print(f"  - {name} í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
    
    try:
        y_pred = model.predict(X_te)
        
        # í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        class_names = getattr(model, "orig_classes_", None)
        if class_names is None:
            class_names = getattr(model, "classes_", None)
        if class_names is None:
            class_names = np.unique(y_te)
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ
        from sklearn.metrics import precision_recall_fscore_support
        
        precision, recall, f1, support = precision_recall_fscore_support(
            y_te, y_pred, labels=class_names, average=None, sample_weight=w_te
        )
        
        # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
        class_accuracy = {}
        for i, class_name in enumerate(class_names):
            class_mask = (y_te == class_name)
            if class_mask.sum() > 0:
                class_accuracy[class_name] = (y_pred[class_mask] == y_te[class_mask]).mean()
            else:
                class_accuracy[class_name] = 0.0
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„
        class_perf_df = pd.DataFrame({
            'class_name': class_names,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'support': support,
            'accuracy': [class_accuracy.get(cls, 0.0) for cls in class_names]
        })
        
        # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_te, y_pred, labels=class_names, sample_weight=w_te)
        cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)
        
        class_performance_results[name] = {
            "model_name": name,
            "class_performance": class_perf_df.to_dict('records'),
            "confusion_matrix": cm_df.to_dict(),
            "overall_metrics": {
                "macro_precision": precision.mean(),
                "macro_recall": recall.mean(),
                "macro_f1": f1.mean(),
                "weighted_precision": np.average(precision, weights=support),
                "weighted_recall": np.average(recall, weights=support),
                "weighted_f1": np.average(f1, weights=support)
            }
        }
        
        # ê°œë³„ CSV íŒŒì¼ë¡œ ì €ì¥
        class_perf_df.to_csv(f"{results_dir}/performance/{name}_class_performance.csv", index=False, encoding='utf-8-sig')
        cm_df.to_csv(f"{results_dir}/performance/{name}_confusion_matrix.csv", encoding='utf-8-sig')
        
    except Exception as e:
        print(f"  - {name} í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

# í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë©”íƒ€ë°ì´í„° ì €ì¥
with open(f"{results_dir}/performance/class_performance_metadata.json", 'w', encoding='utf-8') as f:
    json.dump(class_performance_results, f, ensure_ascii=False, indent=2)

print(f"âœ… í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ ì €ì¥ ì™„ë£Œ: {results_dir}/performance/")

# --------------------------------------------------
# 6) ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (ì „ì²´ ë°ì´í„°)
# --------------------------------------------------
print("6/6: ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì›ë³¸ í”¼ì²˜ë“¤ê³¼ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©
prediction_results = {}

for name, model in basic_models.items():
    print(f"  - {name} ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    try:
        # ì˜ˆì¸¡
        y_pred = model.predict(X_te)
        proba = model.predict_proba(X_te) if hasattr(model, "predict_proba") else None
        
        # í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        class_names = getattr(model, "orig_classes_", None)
        if class_names is None:
            class_names = getattr(model, "classes_", None)
        if class_names is None:
            class_names = [f"class_{i}" for i in range(proba.shape[1])] if proba is not None else []
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        result_df = X_te.copy()
        result_df['actual_class'] = y_te.values
        result_df['predicted_class'] = y_pred
        result_df['prediction_correct'] = (y_te == y_pred)
        result_df['sample_weight'] = w_te.values
        result_df['model'] = name
        
        # ì˜ˆì¸¡ í™•ë¥  ì¶”ê°€
        if proba is not None:
            for i, class_name in enumerate(class_names):
                result_df[f'prob_{class_name}'] = proba[:, i]
            result_df['confidence'] = proba.max(axis=1)
            
            # Top-3 ì˜ˆì¸¡
            top3_indices = np.argsort(proba, axis=1)[:, -3:][:, ::-1]
            top3_classes = class_names[top3_indices]
            top3_probs = np.take_along_axis(proba, top3_indices, axis=1)
            
            result_df['top1_class'] = top3_classes[:, 0]
            result_df['top1_prob'] = top3_probs[:, 0]
            result_df['top2_class'] = top3_classes[:, 1]
            result_df['top2_prob'] = top3_probs[:, 1]
            result_df['top3_class'] = top3_classes[:, 2]
            result_df['top3_prob'] = top3_probs[:, 2]
        
        # ì›ë³¸ ë°ì´í„°ì˜ ì¶”ê°€ ì •ë³´ë„ í¬í•¨
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•´ì„œ ì›ë³¸ ë°ì´í„°ì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        test_indices = X_te.index
        original_data = train.loc[test_indices]
        
        # ì›ë³¸ ë°ì´í„°ì˜ ì¶”ê°€ ì»¬ëŸ¼ë“¤
        for col in ['ë…„ë„', 'êµ¬ë¶„', 'ì§€ì—­', 'ìƒë³‘ì½”ë“œ', 'ì§„ë£Œê³¼', 'count']:
            if col in original_data.columns:
                result_df[f'original_{col}'] = original_data[col].values
        
        prediction_results[name] = {
            "model_name": name,
            "total_predictions": len(result_df),
            "correct_predictions": result_df['prediction_correct'].sum(),
            "accuracy": result_df['prediction_correct'].mean(),
            "class_names": class_names.tolist() if isinstance(class_names, np.ndarray) else class_names
        }
        
        # ê°œë³„ CSV íŒŒì¼ë¡œ ì €ì¥
        result_df.to_csv(f"{results_dir}/predictions/{name}_complete_predictions.csv", index=False, encoding='utf-8-sig')
        
    except Exception as e:
        print(f"  - {name} ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½
all_predictions_summary = pd.DataFrame(prediction_results.values())
all_predictions_summary.to_csv(f"{results_dir}/predictions/all_models_prediction_summary.csv", index=False, encoding='utf-8-sig')

# ì˜ˆì¸¡ ê²°ê³¼ ë©”íƒ€ë°ì´í„° ì €ì¥
with open(f"{results_dir}/predictions/prediction_results_metadata.json", 'w', encoding='utf-8') as f:
    json.dump(prediction_results, f, ensure_ascii=False, indent=2)

print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_dir}/predictions/")

# --------------------------------------------------
# ìµœì¢… ìš”ì•½
# --------------------------------------------------
print("\n" + "="*60)
print("=== ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ ===")
print("="*60)

print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼ë“¤:")
print(f"  ğŸ“Š ì„±ëŠ¥ ë¹„êµ: {results_dir}/performance/")
print(f"    - model_performance_comparison.csv")
print(f"    - performance_metadata.json")
print(f"    - grid_search_results.json")
print(f"    - grid_search_summary.csv")
print(f"    - [ëª¨ë¸ëª…]_class_performance.csv")
print(f"    - [ëª¨ë¸ëª…]_confusion_matrix.csv")
print(f"    - class_performance_metadata.json")
print()
print(f"  ğŸ” í”¼ì²˜ ì¤‘ìš”ë„: {results_dir}/features/")
print(f"    - [ëª¨ë¸ëª…]_feature_importance.csv")
print(f"    - average_feature_importance.csv")
print(f"    - feature_importance_metadata.json")
print()
print(f"  ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼: {results_dir}/predictions/")
print(f"    - [ëª¨ë¸ëª…]_prediction_probabilities.csv")
print(f"    - [ëª¨ë¸ëª…]_complete_predictions.csv")
print(f"    - all_models_prediction_summary.csv")
print(f"    - prediction_probability_metadata.json")
print(f"    - prediction_results_metadata.json")

print(f"\nâœ… ëª¨ë“  ë°ì´í„° ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ğŸ“Š ì´ {len(basic_models)}ê°œ ëª¨ë¸ì˜ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ğŸ¯ ë‹¤ë¥¸ ëª¨ë¸ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ê²°ê³¼ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("="*60)

# --------------------------------------------------
# 10) ìµœì  ëª¨ë¸: ADASYN + Stacking ì¡°í•©
# --------------------------------------------------
print("\n" + "="*60)
print("=== ìµœì  ëª¨ë¸: ADASYN + Stacking ì¡°í•© í•™ìŠµ ===")
print("="*60)

print("ğŸ¯ ADASYN ìƒ˜í”Œë§ê³¼ Stacking ëª¨ë¸ì„ ì¡°í•©í•œ ìµœì  ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤...")
print("ğŸ“Š ì´ ì¡°í•©ì´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.")

# ADASYN ìƒ˜í”Œë§ ì ìš©
print("1/3: ADASYN ìƒ˜í”Œë§ ì ìš© ì¤‘...")
X_tr_preprocessed = preprocessor.transform(X_tr)
X_resampled, y_resampled = ADASYN(random_state=42).fit_resample(X_tr_preprocessed, y_tr)

print(f"  - ì›ë³¸ ë°ì´í„° í¬ê¸°: {X_tr_preprocessed.shape}")
print(f"  - ìƒ˜í”Œë§ í›„ ë°ì´í„° í¬ê¸°: {X_resampled.shape}")
print(f"  - í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
for cls, count in pd.Series(y_resampled).value_counts().items():
    print(f"    {cls}: {count}ê°œ")

# ìƒ˜í”Œë§ í›„ ê°€ì¤‘ì¹˜ ê³„ì‚°
print("2/3: ìƒ˜í”Œë§ í›„ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
le_resampled = LabelEncoder()
y_resampled_encoded = le_resampled.fit_transform(y_resampled)
class_counts_resampled = np.bincount(y_resampled_encoded)
total_samples_resampled = len(y_resampled_encoded)
resampled_weights = (total_samples_resampled / (len(class_counts_resampled) * class_counts_resampled)) ** 1.5
sample_weights_resampled = np.array([resampled_weights[label] for label in y_resampled_encoded])

print(f"  - ê°€ì¤‘ì¹˜ ë²”ìœ„: {resampled_weights.min():.2f} ~ {resampled_weights.max():.2f}")

# Stacking ëª¨ë¸ êµ¬ì„± (ê¸°ì¡´ ëª¨ë¸ë“¤ ì‚¬ìš©)
print("3/3: ADASYN + Stacking ëª¨ë¸ í•™ìŠµ ì¤‘...")

# ê°œë³„ ëª¨ë¸ë“¤ (ê°€ì¤‘ì¹˜ ì ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤)
base_estimators = [
    ('xgb', XGBWrapper(
        eval_metric="mlogloss",
        random_state=42,
        tree_method="hist",
        device="cuda",
        max_bin=256,
        single_precision_histogram=True,
        enable_categorical=False,
        max_leaves=0,
        grow_policy="lossguide",
        n_estimators=400,  # ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©
        max_depth=6,
        learning_rate=0.1,
        reg_alpha=0.1,
        reg_lambda=0.1
    )),
    ('lgb', LGBMClassifier(
        objective="multiclass",
        random_state=42,
        verbose=-1,
        device="gpu",
        gpu_platform_id=0,
        gpu_device_id=0,
        force_col_wise=True,
        gpu_use_dp=False,
        max_bin=255,
        num_leaves=31,
        min_child_samples=20,
        subsample=1.0,
        colsample_bytree=1.0,
        n_jobs=1,
        deterministic=True,
        force_row_wise=False,
        n_estimators=400,  # ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©
        max_depth=6,
        learning_rate=0.1
    )),
    ('rf', RandomForestClassifier(
        class_weight="balanced", 
        random_state=42,
        n_estimators=200,  # ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©
        max_depth=20,
        min_samples_split=5
    )),
    ('gb', GradientBoostingClassifier(
        random_state=42,
        n_estimators=200,  # ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©
        max_depth=6,
        learning_rate=0.1
    ))
]

# Stacking ëª¨ë¸ ìƒì„±
adasyn_stacking = StackingClassifier(
    estimators=base_estimators,
    final_estimator=LogisticRegression(max_iter=5000, random_state=42),
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    n_jobs=1  # GPU ëª¨ë¸ê³¼ í˜¼ì¬ë¡œ ì¸í•´ ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‚¬ìš©
)

print(f"  - ë² ì´ìŠ¤ ëª¨ë¸: {[name for name, _ in base_estimators]}")
print(f"  - ë©”íƒ€ ëª¨ë¸: LogisticRegression")
print(f"  - êµì°¨ê²€ì¦: 5-fold StratifiedKFold")

# GPU ì‚¬ìš©ë¥  í™•ì¸
try:
    result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                          capture_output=True, text=True)
    if result.returncode == 0:
        gpu_util = result.stdout.strip()
        print(f"ğŸ–¥ï¸ í•™ìŠµ ì „ GPU ì‚¬ìš©ë¥ : {gpu_util}%")
except:
    pass

# ëª¨ë¸ í•™ìŠµ
print("  - ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
adasyn_stacking.fit(X_resampled, y_resampled)

# í•™ìŠµ í›„ GPU ì‚¬ìš©ë¥  í™•ì¸
try:
    result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                          capture_output=True, text=True)
    if result.returncode == 0:
        gpu_util = result.stdout.strip()
        print(f"ğŸ–¥ï¸ í•™ìŠµ í›„ GPU ì‚¬ìš©ë¥ : {gpu_util}%")
except:
    pass

print("âœ… ADASYN + Stacking ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# --------------------------------------------------
# 11) ìµœì  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë°ì´í„° ì¶”ì¶œ
# --------------------------------------------------
print("\n" + "="*60)
print("=== ìµœì  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë°ì´í„° ì¶”ì¶œ ===")
print("="*60)

# ì„±ëŠ¥ í‰ê°€
print("1/4: ìµœì  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
X_te_preprocessed = preprocessor.transform(X_te)
y_pred_optimal = adasyn_stacking.predict(X_te_preprocessed)
proba_optimal = adasyn_stacking.predict_proba(X_te_preprocessed)

# ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
acc_optimal = accuracy_score(y_te, y_pred_optimal, sample_weight=w_te)
macro_f1_optimal = f1_score(y_te, y_pred_optimal, average="macro", sample_weight=w_te)
weighted_f1_optimal = f1_score(y_te, y_pred_optimal, average="weighted", sample_weight=w_te)
bal_acc_optimal = balanced_accuracy_score(y_te, y_pred_optimal, sample_weight=w_te)
top3_acc_optimal = top_k_accuracy_score(y_te, proba_optimal, k=3, sample_weight=w_te)

# í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
class_names_optimal = getattr(adasyn_stacking, "classes_", None)
if class_names_optimal is None:
    class_names_optimal = np.unique(y_te)

y_bin_optimal = label_binarize(y_te, classes=class_names_optimal)
roc_auc_optimal = roc_auc_score(y_bin_optimal, proba_optimal, average="macro", sample_weight=w_te)

print(f"âœ… ìµœì  ëª¨ë¸ ì„±ëŠ¥:")
print(f"  - ì •í™•ë„: {acc_optimal:.4f}")
print(f"  - Macro F1: {macro_f1_optimal:.4f}")
print(f"  - Weighted F1: {weighted_f1_optimal:.4f}")
print(f"  - Balanced Accuracy: {bal_acc_optimal:.4f}")
print(f"  - Top-3 Accuracy: {top3_acc_optimal:.4f}")
print(f"  - Macro ROC-AUC: {roc_auc_optimal:.4f}")

# 2. ì„±ëŠ¥ ë¹„êµì— ìµœì  ëª¨ë¸ ì¶”ê°€
print("2/4: ì„±ëŠ¥ ë¹„êµ ë°ì´í„°ì— ìµœì  ëª¨ë¸ ì¶”ê°€ ì¤‘...")

optimal_performance = {
    "model_name": "ADASYN_Stacking",
    "accuracy": acc_optimal,
    "macro_f1": macro_f1_optimal,
    "weighted_f1": weighted_f1_optimal,
    "balanced_accuracy": bal_acc_optimal,
    "top3_accuracy": top3_acc_optimal,
    "top5_accuracy": top_k_accuracy_score(y_te, proba_optimal, k=5, sample_weight=w_te),
    "roc_auc_macro": roc_auc_optimal,
    "roc_auc_weighted": roc_auc_score(y_bin_optimal, proba_optimal, average="weighted", sample_weight=w_te),
    "f1_scores_by_class": f1_score(y_te, y_pred_optimal, average=None, sample_weight=w_te).tolist(),
    "classes": class_names_optimal.tolist()
}

# ê¸°ì¡´ ì„±ëŠ¥ ê²°ê³¼ì— ìµœì  ëª¨ë¸ ì¶”ê°€
performance_results.append(optimal_performance)
performance_df_updated = pd.DataFrame(performance_results)
performance_df_updated.to_csv(f"{results_dir}/performance/model_performance_comparison_with_optimal.csv", index=False, encoding='utf-8-sig')

# 3. ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ì— ìµœì  ëª¨ë¸ ì¶”ê°€
print("3/4: ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ì— ìµœì  ëª¨ë¸ ì¶”ê°€ ì¤‘...")

# ì˜ˆì¸¡ í™•ë¥  ë°ì´í„°í”„ë ˆì„ ìƒì„±
proba_df_optimal = pd.DataFrame(proba_optimal, columns=class_names_optimal)
proba_df_optimal['actual_class'] = y_te.values
proba_df_optimal['predicted_class'] = y_pred_optimal
proba_df_optimal['confidence'] = proba_optimal.max(axis=1)
proba_df_optimal['model'] = 'ADASYN_Stacking'

# Top-3 ì˜ˆì¸¡
top3_indices_optimal = np.argsort(proba_optimal, axis=1)[:, -3:][:, ::-1]
top3_classes_optimal = class_names_optimal[top3_indices_optimal]
top3_probs_optimal = np.take_along_axis(proba_optimal, top3_indices_optimal, axis=1)

proba_df_optimal['top1_class'] = top3_classes_optimal[:, 0]
proba_df_optimal['top1_prob'] = top3_probs_optimal[:, 0]
proba_df_optimal['top2_class'] = top3_classes_optimal[:, 1]
proba_df_optimal['top2_prob'] = top3_probs_optimal[:, 1]
proba_df_optimal['top3_class'] = top3_classes_optimal[:, 2]
proba_df_optimal['top3_prob'] = top3_probs_optimal[:, 2]

# ì‹ ë¢°ë„ êµ¬ê°„ë³„ ë¶„ì„
confidence_bins = [0, 0.5, 0.7, 0.8, 0.9, 1.0]
confidence_labels = ['0-0.5', '0.5-0.7', '0.7-0.8', '0.8-0.9', '0.9-1.0']
proba_df_optimal['confidence_bin'] = pd.cut(proba_df_optimal['confidence'], bins=confidence_bins, labels=confidence_labels)

# ì‹ ë¢°ë„ êµ¬ê°„ë³„ ì •í™•ë„
confidence_accuracy_optimal = proba_df_optimal.groupby('confidence_bin').apply(
    lambda x: (x['actual_class'] == x['predicted_class']).mean()
).reset_index()
confidence_accuracy_optimal.columns = ['confidence_bin', 'accuracy']

# ì˜ˆì¸¡ í™•ë¥  ë©”íƒ€ë°ì´í„°ì— ìµœì  ëª¨ë¸ ì¶”ê°€
prediction_probability_results['ADASYN_Stacking'] = {
    "model_name": "ADASYN_Stacking",
    "class_names": class_names_optimal.tolist(),
    "confidence_accuracy": confidence_accuracy_optimal.to_dict('records'),
    "probability_stats": {
        "mean_confidence": proba_df_optimal['confidence'].mean(),
        "std_confidence": proba_df_optimal['confidence'].std(),
        "min_confidence": proba_df_optimal['confidence'].min(),
        "max_confidence": proba_df_optimal['confidence'].max()
    }
}

# ê°œë³„ CSV íŒŒì¼ë¡œ ì €ì¥
proba_df_optimal.to_csv(f"{results_dir}/predictions/ADASYN_Stacking_prediction_probabilities.csv", index=False, encoding='utf-8-sig')

# 4. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ì— ìµœì  ëª¨ë¸ ì¶”ê°€
print("4/4: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„ì— ìµœì  ëª¨ë¸ ì¶”ê°€ ì¤‘...")

from sklearn.metrics import precision_recall_fscore_support, confusion_matrix

precision_optimal, recall_optimal, f1_optimal, support_optimal = precision_recall_fscore_support(
    y_te, y_pred_optimal, labels=class_names_optimal, average=None, sample_weight=w_te
)

# í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
class_accuracy_optimal = {}
for i, class_name in enumerate(class_names_optimal):
    class_mask = (y_te == class_name)
    if class_mask.sum() > 0:
        class_accuracy_optimal[class_name] = (y_pred_optimal[class_mask] == y_te[class_mask]).mean()
    else:
        class_accuracy_optimal[class_name] = 0.0

# í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„
class_perf_df_optimal = pd.DataFrame({
    'class_name': class_names_optimal,
    'precision': precision_optimal,
    'recall': recall_optimal,
    'f1_score': f1_optimal,
    'support': support_optimal,
    'accuracy': [class_accuracy_optimal.get(cls, 0.0) for cls in class_names_optimal]
})

# í˜¼ë™ í–‰ë ¬ ê³„ì‚°
cm_optimal = confusion_matrix(y_te, y_pred_optimal, labels=class_names_optimal, sample_weight=w_te)
cm_df_optimal = pd.DataFrame(cm_optimal, index=class_names_optimal, columns=class_names_optimal)

# í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë©”íƒ€ë°ì´í„°ì— ìµœì  ëª¨ë¸ ì¶”ê°€
class_performance_results['ADASYN_Stacking'] = {
    "model_name": "ADASYN_Stacking",
    "class_performance": class_perf_df_optimal.to_dict('records'),
    "confusion_matrix": cm_df_optimal.to_dict(),
    "overall_metrics": {
        "macro_precision": precision_optimal.mean(),
        "macro_recall": recall_optimal.mean(),
        "macro_f1": f1_optimal.mean(),
        "weighted_precision": np.average(precision_optimal, weights=support_optimal),
        "weighted_recall": np.average(recall_optimal, weights=support_optimal),
        "weighted_f1": np.average(f1_optimal, weights=support_optimal)
    }
}

# ê°œë³„ CSV íŒŒì¼ë¡œ ì €ì¥
class_perf_df_optimal.to_csv(f"{results_dir}/performance/ADASYN_Stacking_class_performance.csv", index=False, encoding='utf-8-sig')
cm_df_optimal.to_csv(f"{results_dir}/performance/ADASYN_Stacking_confusion_matrix.csv", encoding='utf-8-sig')

# 5. ì™„ì „í•œ ì˜ˆì¸¡ ê²°ê³¼ì— ìµœì  ëª¨ë¸ ì¶”ê°€
print("5/5: ì™„ì „í•œ ì˜ˆì¸¡ ê²°ê³¼ì— ìµœì  ëª¨ë¸ ì¶”ê°€ ì¤‘...")

# ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
result_df_optimal = X_te.copy()
result_df_optimal['actual_class'] = y_te.values
result_df_optimal['predicted_class'] = y_pred_optimal
result_df_optimal['prediction_correct'] = (y_te == y_pred_optimal)
result_df_optimal['sample_weight'] = w_te.values
result_df_optimal['model'] = 'ADASYN_Stacking'

# ì˜ˆì¸¡ í™•ë¥  ì¶”ê°€
for i, class_name in enumerate(class_names_optimal):
    result_df_optimal[f'prob_{class_name}'] = proba_optimal[:, i]
result_df_optimal['confidence'] = proba_optimal.max(axis=1)

# Top-3 ì˜ˆì¸¡
result_df_optimal['top1_class'] = top3_classes_optimal[:, 0]
result_df_optimal['top1_prob'] = top3_probs_optimal[:, 0]
result_df_optimal['top2_class'] = top3_classes_optimal[:, 1]
result_df_optimal['top2_prob'] = top3_probs_optimal[:, 1]
result_df_optimal['top3_class'] = top3_classes_optimal[:, 2]
result_df_optimal['top3_prob'] = top3_probs_optimal[:, 2]

# ì›ë³¸ ë°ì´í„°ì˜ ì¶”ê°€ ì •ë³´ë„ í¬í•¨
test_indices = X_te.index
original_data = train.loc[test_indices]

for col in ['ë…„ë„', 'êµ¬ë¶„', 'ì§€ì—­', 'ìƒë³‘ì½”ë“œ', 'ì§„ë£Œê³¼', 'count']:
    if col in original_data.columns:
        result_df_optimal[f'original_{col}'] = original_data[col].values

# ì˜ˆì¸¡ ê²°ê³¼ ë©”íƒ€ë°ì´í„°ì— ìµœì  ëª¨ë¸ ì¶”ê°€
prediction_results['ADASYN_Stacking'] = {
    "model_name": "ADASYN_Stacking",
    "total_predictions": len(result_df_optimal),
    "correct_predictions": result_df_optimal['prediction_correct'].sum(),
    "accuracy": result_df_optimal['prediction_correct'].mean(),
    "class_names": class_names_optimal.tolist() if isinstance(class_names_optimal, np.ndarray) else class_names_optimal
}

# ê°œë³„ CSV íŒŒì¼ë¡œ ì €ì¥
result_df_optimal.to_csv(f"{results_dir}/predictions/ADASYN_Stacking_complete_predictions.csv", index=False, encoding='utf-8-sig')

# ì—…ë°ì´íŠ¸ëœ ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½
all_predictions_summary_updated = pd.DataFrame(prediction_results.values())
all_predictions_summary_updated.to_csv(f"{results_dir}/predictions/all_models_prediction_summary_with_optimal.csv", index=False, encoding='utf-8-sig')

# ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° ì €ì¥
with open(f"{results_dir}/performance/performance_metadata_with_optimal.json", 'w', encoding='utf-8') as f:
    json.dump({
        "timestamp": datetime.now().isoformat(),
        "test_data_size": len(X_te),
        "train_data_size": len(X_tr),
        "num_classes": len(np.unique(y_te)),
        "class_distribution": y_te.value_counts().to_dict(),
        "models": performance_results,
        "optimal_model": {
            "name": "ADASYN_Stacking",
            "sampling_method": "ADASYN",
            "base_models": [name for name, _ in base_estimators],
            "meta_model": "LogisticRegression",
            "performance": optimal_performance
        }
    }, f, ensure_ascii=False, indent=2)

with open(f"{results_dir}/predictions/prediction_probability_metadata_with_optimal.json", 'w', encoding='utf-8') as f:
    json.dump(prediction_probability_results, f, ensure_ascii=False, indent=2)

with open(f"{results_dir}/performance/class_performance_metadata_with_optimal.json", 'w', encoding='utf-8') as f:
    json.dump(class_performance_results, f, ensure_ascii=False, indent=2)

with open(f"{results_dir}/predictions/prediction_results_metadata_with_optimal.json", 'w', encoding='utf-8') as f:
    json.dump(prediction_results, f, ensure_ascii=False, indent=2)

# --------------------------------------------------
# ìµœì¢… ìš”ì•½
# --------------------------------------------------
print("\n" + "="*60)
print("=== ìµœì  ëª¨ë¸ ì¶”ê°€ ì™„ë£Œ ===")
print("="*60)

print(f"ğŸ¯ ìµœì  ëª¨ë¸: ADASYN + Stacking")
print(f"ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
print(f"  - ì •í™•ë„: {acc_optimal:.4f}")
print(f"  - Macro F1: {macro_f1_optimal:.4f}")
print(f"  - Top-3 ì •í™•ë„: {top3_acc_optimal:.4f}")
print(f"  - ROC-AUC: {roc_auc_optimal:.4f}")

print(f"\nğŸ“ ì¶”ê°€ëœ íŒŒì¼ë“¤:")
print(f"  ğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
print(f"    - model_performance_comparison_with_optimal.csv")
print(f"    - performance_metadata_with_optimal.json")
print(f"  ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:")
print(f"    - ADASYN_Stacking_prediction_probabilities.csv")
print(f"    - ADASYN_Stacking_complete_predictions.csv")
print(f"    - ADASYN_Stacking_class_performance.csv")
print(f"    - ADASYN_Stacking_confusion_matrix.csv")
print(f"    - all_models_prediction_summary_with_optimal.csv")

print(f"\nâœ… ìµœì  ëª¨ë¸ì´ ëª¨ë“  ë°ì´í„° ì¶”ì¶œì— í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ğŸ¯ ADASYN_Stacking_complete_predictions.csv íŒŒì¼ì—ì„œ ìµœì  ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
print("="*60)