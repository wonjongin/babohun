import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report


# 1) ì›ë³¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
ekqlseh = pd.read_csv(
    "C:/Users/jenny/babohun/new_merged_data/ë‹¤ë¹ˆë„ ì§ˆí™˜ í™˜ì ì—°ë ¹ë³„ ë¶„í¬_ìˆœìœ„ì¶”ê°€_í•©ê³„ê³„ì‚°_ê°’í†µì¼.csv",
    encoding="utf-8-sig"
)

# 2) ì™¸ë˜ì¸ì› ì»¬ëŸ¼ ë³‘í•©, ì…ì›(ì‹¤ì¸ì›) í–‰ ì œê±°
ekqlseh.loc[ekqlseh['êµ¬ë¶„'].str.contains('ì™¸ë˜'), 'ì—°ì¸ì›'] = ekqlseh['ì‹¤ì¸ì›']
ekqlseh = ekqlseh[~(ekqlseh['êµ¬ë¶„'] == 'ì…ì›(ì‹¤ì¸ì›)')]

# 3) ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
df_result = ekqlseh.drop(columns=['ìˆœìœ„', 'ìƒë³‘ëª…', 'ì‹¤ì¸ì›'])

# 4) ì§€ì—­ í•„í„°ë§
exclude_regions = ['ì„œìš¸', 'ëŒ€ì „', 'ëŒ€êµ¬']
df_filtered = df_result[~df_result['ì§€ì—­'].isin(exclude_regions)].copy()

# 5) ì»¬ëŸ¼ëª… ë³€ê²½: 'ì§„ë£Œë¹„(ì²œì›)' â†’ 'ì§„ë£Œë¹„'
df_filtered.rename(columns={'ì§„ë£Œë¹„(ì²œì›)': 'ì§„ë£Œë¹„'}, inplace=True)

# 6) ìƒë³‘ì½”ë“œ â†” ì§„ë£Œê³¼ ë§¤í•‘ í…Œì´ë¸” ë¶ˆëŸ¬ì˜¤ê¸° ë° ë³‘í•©
mapping = pd.read_csv(
    "C:/Users/jenny/babohun/df_result2_with_ì‹¬í‰ì›.csv",
    encoding="utf-8-sig"
)
df_filtered = (
    df_filtered
    .merge(mapping[['ìƒë³‘ì½”ë“œ', 'ì§„ë£Œê³¼']], on='ìƒë³‘ì½”ë“œ', how='left')
    .dropna(subset=['ì§„ë£Œê³¼'])  # ë§¤í•‘ ì•ˆ ëœ í–‰ ì œê±°
)


# 3) ê³ ë¹„ìš© ì—¬ë¶€ ë ˆì´ë¸” ìƒì„± (ìƒìœ„ 25%)
threshold = df['ì§„ë£Œë¹„'].quantile(0.75)
df['high_cost'] = (df['ì§„ë£Œë¹„'] >= threshold).astype(int)

# 4) í”¼ì²˜ ì¤€ë¹„: ì§„ë£Œê³¼ ì›-í•«ë§Œ ì‚¬ìš©
X = pd.get_dummies(df['ì§„ë£Œê³¼'], prefix='dept')
y = df['high_cost']

# 5) í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.3, random_state=42
)

# 6) ëª¨ë¸ í•™ìŠµ & ì¤‘ìš”ë„ ì¶”ì¶œ í•¨ìˆ˜
def fit_and_report(model, name):
    model.fit(X_train, y_train)
    print(f"\n=== {name} ===")
    print(classification_report(y_test, model.predict(X_test)))
    imps = pd.Series(model.feature_importances_, index=X.columns)
    top = imps.nlargest(10)
    print(f"{name} ì¤‘ìš” ì§„ë£Œê³¼ Top 10:\n{top}\n")
    return top

# 7) Decision Tree
dt = DecisionTreeClassifier(max_depth=4, class_weight='balanced', random_state=42)
top_dt = fit_and_report(dt, "Decision Tree")

# 8) Random Forest
rf = RandomForestClassifier(
    n_estimators=200, max_depth=6,
    class_weight='balanced', random_state=42, n_jobs=-1
)
top_rf = fit_and_report(rf, "Random Forest")

# 9) Gradient Boosting
gb = GradientBoostingClassifier(
    n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42
)
top_gb = fit_and_report(gb, "Gradient Boosting")

# 10) ìµœì¢… â€œìƒìœ„ì§„ë£Œë¹„ ìœ ë°œ ì§„ë£Œê³¼â€ ì„ ì •
# ì„¸ ëª¨ë¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì¤‘ìš”ë„ê°€ ë†’ì•˜ë˜ ì§„ë£Œê³¼ë¥¼ ë½‘ì•„ë´…ë‹ˆë‹¤.
common = set(top_dt.index) & set(top_rf.index) & set(top_gb.index)
print("ğŸš€ ìµœì¢… ìƒìœ„ì§„ë£Œë¹„ ìœ ë°œ ì§„ë£Œê³¼ (ê³µí†µ Top ì¤‘ìš”ë„):", common)
