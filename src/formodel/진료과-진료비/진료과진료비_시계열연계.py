# -*- coding: utf-8 -*-
"""
Created on Sat Jun 28 20:43:03 2025
author: jenny

ìƒë³‘ì½”ë“œ/ì§€ì—­ ê¸°ë°˜ + ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„° ì—°ê³„
 1) ë¹„ëª¨ìˆ˜ ê²€ì •
 2) ê³ ë¹„ìš© ì—¬ë¶€ ë¶„ë¥˜ ëª¨ë¸
 3) ì§„ë£Œë¹„ íšŒê·€ ëª¨ë¸
 4) LightGBM íšŒê·€ ëª¨ë¸ (ë¡œê·¸ íƒ€ê¹ƒ + CV)
 5) ë¡œê·¸ ìŠ¤ì¼€ì¼ ê¸°ë°˜ ì§„ë£Œë¹„ êµ¬ê°„ ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
import scikit_posthocs as sp
import lightgbm as lgb
import joblib
import os

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier,
    RandomForestRegressor, GradientBoostingRegressor
)
from sklearn.metrics import (
    classification_report,
    mean_absolute_error, mean_squared_error, r2_score
)

model_performance = []

def calculate_classification_metrics(y_true, y_pred, model_name):
    """ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    
    return {
        'model_name': model_name,
        'model_type': 'classification',
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }

def calculate_regression_metrics(y_true, y_pred, model_name):
    """íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    return {
        'model_name': model_name,
        'model_type': 'regression',
        'mae': mae,
        'rmse': rmse,
        'r2_score': r2
    } 

# ----------------------------------------------------------------------
# 1) ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬
# ----------------------------------------------------------------------
print("=== ë°ì´í„° ë¡œë”© ì‹œì‘ ===")

data_csv = "final_merged_data/ë‹¤ë¹ˆë„ ì§ˆí™˜ í™˜ì ì—°ë ¹ë³„ ë¶„í¬.csv"
mapping_csv = "new_merged_data/df_result2_with_ì‹¬í‰ì›.csv"

# ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„° ë¡œë“œ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´)
try:
    df_pred = pd.read_csv('analysis_data/ë³‘ì›ë³„_ì§„ë£Œê³¼ë³„_ì…ì›ì™¸ë˜_í†µí•©_ì‹œê³„ì—´ì˜ˆì¸¡ê²°ê³¼_ê°œì„ .csv')
    print("âœ… ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
except:
    print("âš ï¸ ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„° ì—†ìŒ")
    df_pred = None

ekqlseh = pd.read_csv(data_csv, encoding="utf-8-sig")
ekqlseh.loc[ekqlseh['êµ¬ë¶„'].str.contains('ì™¸ë˜'), 'ì—°ì¸ì›'] = ekqlseh['ì‹¤ì¸ì›']
ekqlseh = ekqlseh[ekqlseh['êµ¬ë¶„'] != 'ì…ì›(ì‹¤ì¸ì›)']

df = ekqlseh.drop(columns=['ìˆœìœ„', 'ìƒë³‘ëª…', 'ì‹¤ì¸ì›'])
df = df[~df['ì§€ì—­'].isin(['ì„œìš¸', 'ëŒ€ì „', 'ëŒ€êµ¬'])].copy()

# ì¤‘ë³µ í™•ì¸ ë° ì œê±°
print(f"ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: {len(df)}")
print(f"ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")
df = df.drop_duplicates()
print(f"ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")

mapping = pd.read_csv(mapping_csv, encoding="utf-8-sig")
df = df.merge(mapping[['ìƒë³‘ì½”ë“œ', 'ì§„ë£Œê³¼']], on='ìƒë³‘ì½”ë“œ', how='left')
df.dropna(subset=['ì§„ë£Œê³¼'], inplace=True)
print(f"ì§„ë£Œê³¼ ë§¤í•‘ í›„ í–‰ ìˆ˜: {len(df)}")

# ----------------------------------------------------------------------
# 2) ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ í”¼ì²˜ ìƒì„±
# ----------------------------------------------------------------------
print("=== ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„° ê¸°ë°˜ í”¼ì²˜ ìƒì„± ===")

if df_pred is not None:
    # ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„°ì—ì„œ ì§„ë£Œê³¼ë³„ í‰ê· ê°’ ê³„ì‚° (ì—°ë„ ë¬´ì‹œ)
    pred_summary = df_pred.groupby(['ì§„ë£Œê³¼']).agg({
        'ARIMAì˜ˆì¸¡': 'mean',
        'RFì˜ˆì¸¡': 'mean',
        'XGBì˜ˆì¸¡': 'mean',
        'ì‹¤ì œê°’': 'mean'
    }).reset_index()
    
    pred_summary.columns = ['ì§„ë£Œê³¼', 'ARIMAì˜ˆì¸¡_í‰ê· ', 'RFì˜ˆì¸¡_í‰ê· ', 'XGBì˜ˆì¸¡_í‰ê· ', 'ì‹¤ì œê°’_í‰ê· ']
    
    # ì§„ë£Œê³¼ë³„ë¡œ ë§¤í•‘ (ì¤‘ë³µ ë°©ì§€)
    print(f"ì‹œê³„ì—´ ë°ì´í„° merge ì „ í–‰ ìˆ˜: {len(df)}")
    print(f"ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„° ì§„ë£Œê³¼ ìˆ˜: {len(pred_summary)}")

    # merge ì „ ì¤‘ë³µ í™•ì¸
    print(f"merge ì „ ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")

    df = df.merge(pred_summary, on='ì§„ë£Œê³¼', how='left')

    # merge í›„ ì¤‘ë³µ í™•ì¸
    print(f"ì‹œê³„ì—´ ë°ì´í„° merge í›„ í–‰ ìˆ˜: {len(df)}")
    print(f"merge í›„ ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")

    # ì¤‘ë³µì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•©ë§Œ ìœ ì§€
    if df.duplicated().sum() > len(df) * 0.5:  # 50% ì´ìƒ ì¤‘ë³µì´ë©´
        print("ì¤‘ë³µì´ ë„ˆë¬´ ë§ì•„ ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•©ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.")
        # ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ëŠ” ì»¬ëŸ¼ ì‚¬ìš©
        available_keys = ['ìƒë³‘ì½”ë“œ', 'ì§€ì—­', 'êµ¬ë¶„', 'ì—°ì¸ì›', 'ì§„ë£Œë¹„(ì²œì›)', 'ì§„ë£Œê³¼']
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
        existing_keys = [key for key in available_keys if key in df.columns]
        print(f"ì‚¬ìš©í•  ì»¬ëŸ¼: {existing_keys}")
        df = df.drop_duplicates(subset=existing_keys)
        print(f"ê³ ìœ  ì¡°í•© ê¸°ì¤€ ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")
    else:
        if df.duplicated().sum() > 0:
            print("ì¼ë°˜ì ì¸ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            df = df.drop_duplicates()
            print(f"ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")
    
    # ì˜ˆì¸¡ê°’ ê´€ë ¨ í”¼ì²˜ ìƒì„±
    df['ì˜ˆì¸¡ê°’_í‰ê· '] = df[['ARIMAì˜ˆì¸¡_í‰ê· ', 'RFì˜ˆì¸¡_í‰ê· ', 'XGBì˜ˆì¸¡_í‰ê· ']].mean(axis=1)
    df['ì˜ˆì¸¡ê°’_í‘œì¤€í¸ì°¨'] = df[['ARIMAì˜ˆì¸¡_í‰ê· ', 'RFì˜ˆì¸¡_í‰ê· ', 'XGBì˜ˆì¸¡_í‰ê· ']].std(axis=1)
    df['ê°€ì¤‘ì˜ˆì¸¡ê°’'] = (0.2 * df['ARIMAì˜ˆì¸¡_í‰ê· '] + 0.3 * df['RFì˜ˆì¸¡_í‰ê· '] + 0.5 * df['XGBì˜ˆì¸¡_í‰ê· '])
    
    # ì˜ˆì¸¡ ì •í™•ë„ ì§€í‘œ
    df['ARIMA_ì˜¤ì°¨'] = abs(df['ARIMAì˜ˆì¸¡_í‰ê· '] - df['ì‹¤ì œê°’_í‰ê· '])
    df['RF_ì˜¤ì°¨'] = abs(df['RFì˜ˆì¸¡_í‰ê· '] - df['ì‹¤ì œê°’_í‰ê· '])
    df['XGB_ì˜¤ì°¨'] = abs(df['XGBì˜ˆì¸¡_í‰ê· '] - df['ì‹¤ì œê°’_í‰ê· '])
    
    # ë¡œê·¸ ë³€í™˜
    df['ARIMAì˜ˆì¸¡_log'] = np.log1p(np.abs(df['ARIMAì˜ˆì¸¡_í‰ê· ']))
    df['RFì˜ˆì¸¡_log'] = np.log1p(np.abs(df['RFì˜ˆì¸¡_í‰ê· ']))
    df['XGBì˜ˆì¸¡_log'] = np.log1p(np.abs(df['XGBì˜ˆì¸¡_í‰ê· ']))
    df['ì‹¤ì œê°’_log'] = np.log1p(np.abs(df['ì‹¤ì œê°’_í‰ê· ']))
    
    # ë¹„ìœ¨ í”¼ì²˜
    df['ARIMA_ë¹„ìœ¨'] = np.where(df['ì‹¤ì œê°’_í‰ê· '] != 0, df['ARIMAì˜ˆì¸¡_í‰ê· '] / df['ì‹¤ì œê°’_í‰ê· '], 1.0)
    df['RF_ë¹„ìœ¨'] = np.where(df['ì‹¤ì œê°’_í‰ê· '] != 0, df['RFì˜ˆì¸¡_í‰ê· '] / df['ì‹¤ì œê°’_í‰ê· '], 1.0)
    df['XGB_ë¹„ìœ¨'] = np.where(df['ì‹¤ì œê°’_í‰ê· '] != 0, df['XGBì˜ˆì¸¡_í‰ê· '] / df['ì‹¤ì œê°’_í‰ê· '], 1.0)
    
    # NaN ê°’ ì²˜ë¦¬
    df = df.fillna(0)
    
    print(f"ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ í”¼ì²˜ ìƒì„± ì™„ë£Œ")
    print(f"ì¶”ê°€ëœ í”¼ì²˜ ìˆ˜: {len(['ì˜ˆì¸¡ê°’_í‰ê· ', 'ì˜ˆì¸¡ê°’_í‘œì¤€í¸ì°¨', 'ê°€ì¤‘ì˜ˆì¸¡ê°’', 'ARIMA_ì˜¤ì°¨', 'RF_ì˜¤ì°¨', 'XGB_ì˜¤ì°¨', 'ARIMAì˜ˆì¸¡_log', 'RFì˜ˆì¸¡_log', 'XGBì˜ˆì¸¡_log', 'ì‹¤ì œê°’_log', 'ARIMA_ë¹„ìœ¨', 'RF_ë¹„ìœ¨', 'XGB_ë¹„ìœ¨'])}ê°œ")
    print(f"ë°ì´í„° í–‰ ìˆ˜: {len(df)}ê°œ")
else:
    # ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
    df['ARIMAì˜ˆì¸¡_í‰ê· '] = 0
    df['RFì˜ˆì¸¡_í‰ê· '] = 0
    df['XGBì˜ˆì¸¡_í‰ê· '] = 0
    df['ì‹¤ì œê°’_í‰ê· '] = 0
    df['ì˜ˆì¸¡ê°’_í‰ê· '] = 0
    df['ì˜ˆì¸¡ê°’_í‘œì¤€í¸ì°¨'] = 0
    df['ê°€ì¤‘ì˜ˆì¸¡ê°’'] = 0
    df['ARIMA_ì˜¤ì°¨'] = 0
    df['RF_ì˜¤ì°¨'] = 0
    df['XGB_ì˜¤ì°¨'] = 0
    df['ARIMAì˜ˆì¸¡_log'] = 0
    df['RFì˜ˆì¸¡_log'] = 0
    df['XGBì˜ˆì¸¡_log'] = 0
    df['ì‹¤ì œê°’_log'] = 0
    df['ARIMA_ë¹„ìœ¨'] = 1.0
    df['RF_ë¹„ìœ¨'] = 1.0
    df['XGB_ë¹„ìœ¨'] = 1.0

# ----------------------------------------------------------------------
# 3) ë¹„ëª¨ìˆ˜ ê²€ì •: Kruskalâ€“Wallis + Dunn's
# ----------------------------------------------------------------------
groups = [g['ì§„ë£Œë¹„(ì²œì›)'].values for _, g in df.groupby('ìƒë³‘ì½”ë“œ') if len(g) >= 3]
H, p = stats.kruskal(*groups)
print(f"=== Kruskalâ€“Wallis ê²€ì •: H={H:.4f}, p-value={p:.4e} ===")

dunn = sp.posthoc_dunn(df, val_col='ì§„ë£Œë¹„(ì²œì›)', group_col='ìƒë³‘ì½”ë“œ', p_adjust='bonferroni')
print("=== Dunn's post-hoc (Bonferroni) ===")
print(dunn)

# ----------------------------------------------------------------------
# 4) ë¶„ë¥˜ ëª¨ë¸: ê³ ë¹„ìš© ì—¬ë¶€ ì˜ˆì¸¡ (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨)
# ----------------------------------------------------------------------
thr = df['ì§„ë£Œë¹„(ì²œì›)'].quantile(0.75)
df['high_cost'] = (df['ì§„ë£Œë¹„(ì²œì›)'] >= thr).astype(int)

# Decision Tree (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨)
X_dt = pd.get_dummies(df[['ìƒë³‘ì½”ë“œ', 'ì˜ˆì¸¡ê°’_í‰ê· ', 'ê°€ì¤‘ì˜ˆì¸¡ê°’']], prefix='', prefix_sep='')
y = df['high_cost']
X_tr_dt, X_te_dt, y_tr, y_te = train_test_split(
    X_dt, y, test_size=0.3, random_state=42, stratify=y
)
dt = DecisionTreeClassifier(max_depth=4, class_weight='balanced', random_state=42)
dt.fit(X_tr_dt, y_tr)
y_pred_dt = dt.predict(X_te_dt)
print("\n=== DecisionTreeClassifier (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨) ===")
print(classification_report(y_te, y_pred_dt))

# ì„±ëŠ¥ ì €ì¥
dt_performance = calculate_classification_metrics(y_te, y_pred_dt, "DecisionTree_Classification")
model_performance.append(dt_performance)

# RandomForest & GradientBoosting (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨)
X_rf = pd.get_dummies(df[['ìƒë³‘ì½”ë“œ', 'ì§€ì—­', 'ì˜ˆì¸¡ê°’_í‰ê· ', 'ê°€ì¤‘ì˜ˆì¸¡ê°’', 'ARIMA_ì˜¤ì°¨', 'RF_ì˜¤ì°¨', 'XGB_ì˜¤ì°¨']], dtype=int)
X_tr_rf, X_te_rf, _, _ = train_test_split(
    X_rf, y, test_size=0.3, random_state=42, stratify=y
)
rf = RandomForestClassifier(
    n_estimators=200, max_depth=6,
    class_weight='balanced', random_state=42, n_jobs=-1
)
gb = GradientBoostingClassifier(
    n_estimators=200, learning_rate=0.05,
    max_depth=4, random_state=42
)
rf.fit(X_tr_rf, y_tr)
gb.fit(X_tr_rf, y_tr)
y_pred_rf = rf.predict(X_te_rf)
y_pred_gb = gb.predict(X_te_rf)
print("\n=== RandomForestClassifier (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨) ===")
print(classification_report(y_te, y_pred_rf))
print("\n=== GradientBoostingClassifier (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨) ===")
print(classification_report(y_te, y_pred_gb))

# ì„±ëŠ¥ ì €ì¥
rf_performance = calculate_classification_metrics(y_te, y_pred_rf, "RandomForest_Classification")
gb_performance = calculate_classification_metrics(y_te, y_pred_gb, "GradientBoosting_Classification")
model_performance.extend([rf_performance, gb_performance])

# ----------------------------------------------------------------------
# 5) íšŒê·€ ëª¨ë¸: ì§„ë£Œë¹„ ì§ì ‘ ì˜ˆì¸¡ (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨)
# ----------------------------------------------------------------------
X_reg = X_rf.copy()
y_reg = df['ì§„ë£Œë¹„(ì²œì›)'].values
X_tr_rg, X_te_rg, y_tr_rg, y_te_rg = train_test_split(
    X_reg, y_reg, test_size=0.3, random_state=42
)

dtr = DecisionTreeRegressor(max_depth=6, random_state=42)
rfr = RandomForestRegressor(n_estimators=200, max_depth=8, random_state=42, n_jobs=-1)
gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)
for m in (dtr, rfr, gbr):
    m.fit(X_tr_rg, y_tr_rg)
print("\n=== íšŒê·€ ëª¨ë¸ í‰ê°€ (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨) ===")
for name, m in [("DT", dtr), ("RF", rfr), ("GB", gbr)]:
    pred = m.predict(X_te_rg)
    print(f"{name} â†’ MAE: {mean_absolute_error(y_te_rg, pred):.0f}ì²œì›, RMSE: {np.sqrt(mean_squared_error(y_te_rg, pred)):.0f}ì²œì›")
    
    # ì„±ëŠ¥ ì €ì¥
    reg_performance = calculate_regression_metrics(y_te_rg, pred, f"{name}_Regression")
    model_performance.append(reg_performance)

# ----------------------------------------------------------------------
# 6) ë¡œê·¸ ìŠ¤ì¼€ì¼ ê¸°ë°˜ ì§„ë£Œë¹„ êµ¬ê°„ ì˜ˆì¸¡ (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨)
# ----------------------------------------------------------------------
# 6.1) ë¡œê·¸ ìŠ¤ì¼€ì¼ êµ¬ê°„ ì •ì˜
min_v = df['ì§„ë£Œë¹„(ì²œì›)'].min()
max_v = df['ì§„ë£Œë¹„(ì²œì›)'].max()
bins = np.logspace(np.log10(min_v), np.log10(max_v), num=6)
# 6.2) êµ¬ê°„ í´ë˜ìŠ¤ í• ë‹¹
labels = pd.cut(df['ì§„ë£Œë¹„(ì²œì›)'], bins=bins, labels=False, include_lowest=True)
# 6.3) NaN & í¬ê·€ êµ¬ê°„ ì œê±°
valid_idx = labels.dropna().index
counts = labels.loc[valid_idx].value_counts().sort_index()
rare = counts[counts < 2].index
use_idx = valid_idx.difference(labels[labels.isin(rare)].index)
X_clean = X_reg.loc[use_idx]
y_clean = labels.loc[use_idx]
# 6.4) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_tr, X_te, y_tr, y_te = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42, stratify=y_clean)
# 6.5) ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥
lgb_clf = lgb.LGBMClassifier(objective='multiclass', num_class=len(y_clean.unique()), learning_rate=0.05, n_estimators=200, num_leaves=31, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, verbosity=-1, seed=42)
lgb_clf.fit(X_tr, y_tr)
y_pred = lgb_clf.predict(X_te)
print("\n=== ë¡œê·¸ ìŠ¤ì¼€ì¼ êµ¬ê°„ ë¶„ë¥˜ ì„±ëŠ¥ (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨) ===")
print(classification_report(y_te, y_pred))

# LightGBM ì„±ëŠ¥ ì €ì¥
lgb_performance = calculate_classification_metrics(y_te, y_pred, "LightGBM_Classification")
model_performance.append(lgb_performance)

# 6.6) ëŒ€í‘œ ì§„ë£Œë¹„ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_cost_bin(code, region, model, feat_cols, bins, pred_features=None):
    """
    ìƒë³‘ì½”ë“œì™€ ì§€ì—­ìœ¼ë¡œ ë¡œê·¸ ìŠ¤ì¼€ì¼ êµ¬ê°„ í´ë˜ìŠ¤ì™€
    ëŒ€í‘œ ì§„ë£Œë¹„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨)
    """
    # ì…ë ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì›-í•« ì¸ì½”ë”©
    df_in = pd.DataFrame([{'ìƒë³‘ì½”ë“œ': code, 'ì§€ì—­': region}])
    
    # ì‹œê³„ì—´ ì˜ˆì¸¡ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if pred_features is not None:
        for key, value in pred_features.items():
            df_in[key] = value
    
    X_in = pd.get_dummies(df_in, columns=['ìƒë³‘ì½”ë“œ', 'ì§€ì—­'], dtype=int)
    X_in = X_in.reindex(columns=feat_cols, fill_value=0)

    # í´ë˜ìŠ¤ ì˜ˆì¸¡
    bin_pred_raw = model.predict(X_in)[0]
    bin_idx = int(bin_pred_raw)

    # í´ë˜ìŠ¤ë³„ ëŒ€í‘œê°’ ì¶”ì¶œ (ì¤‘ì•™ê°’)
    midpoint = (bins[bin_idx] + bins[bin_idx + 1]) / 2
    return bin_idx, midpoint

# ì˜ˆì‹œ ì‚¬ìš©:
feat_cols = X_reg.columns.tolist()
example_code, example_region = 'M48', 'ë¶€ì‚°'
# ì‹œê³„ì—´ ì˜ˆì¸¡ê°’ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” í•´ë‹¹ ìƒë³‘ì½”ë“œì˜ ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©)
pred_features = {
    'ì˜ˆì¸¡ê°’_í‰ê· ': 1000,
    'ê°€ì¤‘ì˜ˆì¸¡ê°’': 950,
    'ARIMA_ì˜¤ì°¨': 50,
    'RF_ì˜¤ì°¨': 30,
    'XGB_ì˜¤ì°¨': 20
}
bin_label, est_cost = predict_cost_bin(
    example_code, example_region,
    lgb_clf, feat_cols, bins, pred_features
)
print(f"ì˜ˆì¸¡ êµ¬ê°„: {bin_label}, ëŒ€í‘œ ì§„ë£Œë¹„: {est_cost:.0f}ì²œì›")

# ----------------------------------------------------------------------
# 7) ê²°ê³¼ ì €ì¥
# ----------------------------------------------------------------------
print("\n=== ê²°ê³¼ ì €ì¥ ì‹œì‘ ===")

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
results_dir = "model_results_ì§„ë£Œê³¼ì§„ë£Œë¹„_ì‹œê³„ì—´"
os.makedirs(results_dir, exist_ok=True)

# ëª¨ë¸ ì €ì¥
joblib.dump(dt, f"{results_dir}/dt_highcost_model_timeseries.pkl")
joblib.dump(rf, f"{results_dir}/rf_highcost_model_timeseries.pkl")
joblib.dump(gb, f"{results_dir}/gb_highcost_model_timeseries.pkl")
joblib.dump(dtr, f"{results_dir}/dtr_cost_regressor_timeseries.pkl")
joblib.dump(rfr, f"{results_dir}/rfr_cost_regressor_timeseries.pkl")
joblib.dump(gbr, f"{results_dir}/gbr_cost_regressor_timeseries.pkl")
joblib.dump(lgb_clf, f"{results_dir}/lgb_cost_bin_classifier_timeseries.pkl")

# ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡
preds = []
for _, row in df.iterrows():
    # í•´ë‹¹ ìƒë³‘ì½”ë“œì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°
    pred_features = {
        'ì˜ˆì¸¡ê°’_í‰ê· ': row.get('ì˜ˆì¸¡ê°’_í‰ê· ', 0),
        'ê°€ì¤‘ì˜ˆì¸¡ê°’': row.get('ê°€ì¤‘ì˜ˆì¸¡ê°’', 0),
        'ARIMA_ì˜¤ì°¨': row.get('ARIMA_ì˜¤ì°¨', 0),
        'RF_ì˜¤ì°¨': row.get('RF_ì˜¤ì°¨', 0),
        'XGB_ì˜¤ì°¨': row.get('XGB_ì˜¤ì°¨', 0)
    }
    
    try:
        bin_label, est_cost = predict_cost_bin(
            row['ìƒë³‘ì½”ë“œ'], row['ì§€ì—­'],
            lgb_clf, feat_cols, bins, pred_features
        )
        preds.append((bin_label, est_cost))
    except:
        preds.append((0, 0))

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ dfì— ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
df['pred_bin_timeseries'], df['pred_cost_timeseries'] = zip(*preds)

# ìµœì¢… ì¤‘ë³µ í™•ì¸ ë° ì œê±°
print(f"ì˜ˆì¸¡ ì™„ë£Œ í›„ ë°ì´í„° í–‰ ìˆ˜: {len(df)}")
print(f"ìµœì¢… ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")
if df.duplicated().sum() > 0:
    print("ìµœì¢… ì¤‘ë³µ í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.")
    df = df.drop_duplicates()
    print(f"ìµœì¢… ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")

# CSVë¡œ ì €ì¥
output_path = f"{results_dir}/ì§„ë£Œë¹„_êµ¬ê°„ì˜ˆì¸¡ê²°ê³¼_ì‹œê³„ì—´ì—°ê³„.csv"
df.to_csv(output_path, index=False, encoding='utf-8-sig')

print(f"ì˜ˆì¸¡ ê²°ê³¼ë¥¼ '{output_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
print(f"ëª¨ë“  ê²°ê³¼ê°€ '{results_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!") 

# ----------------------------------------------------------------------
# 8) ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ë° ì €ì¥
# ----------------------------------------------------------------------
print("\n" + "="*60)
print("=== ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½ (ì‹œê³„ì—´ ë°ì´í„° í¬í•¨) ===")
print("="*60)

# ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
for perf in model_performance:
    print(f"\nğŸ“Š {perf['model_name']} ({perf['model_type']})")
    if perf['model_type'] == 'classification':
        print(f"   ì •í™•ë„: {perf['accuracy']:.4f}")
        print(f"   ì •ë°€ë„: {perf['precision']:.4f}")
        print(f"   ì¬í˜„ìœ¨: {perf['recall']:.4f}")
        print(f"   F1ì ìˆ˜: {perf['f1_score']:.4f}")
    else:  # regression
        print(f"   MAE: {perf['mae']:.0f}ì²œì›")
        print(f"   RMSE: {perf['rmse']:.0f}ì²œì›")
        print(f"   RÂ²: {perf['r2_score']:.4f}")

# ì„±ëŠ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° CSV ì €ì¥
performance_df = pd.DataFrame(model_performance)
performance_csv_path = f"{results_dir}/ëª¨ë¸ë³„_ì„±ëŠ¥_ìš”ì•½_ì‹œê³„ì—´ì—°ê³„.csv"
performance_df.to_csv(performance_csv_path, index=False, encoding='utf-8-sig')

print(f"\nâœ… ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ì´ '{performance_csv_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
if len(model_performance) > 0:
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
    
    # ë¶„ë¥˜ ëª¨ë¸ ì¤‘ ìµœê³  F1 ì ìˆ˜
    classification_models = [p for p in model_performance if p['model_type'] == 'classification']
    if classification_models:
        best_classification = max(classification_models, key=lambda x: x['f1_score'])
        print(f"   ë¶„ë¥˜ ëª¨ë¸: {best_classification['model_name']} (F1: {best_classification['f1_score']:.4f})")
    
    # íšŒê·€ ëª¨ë¸ ì¤‘ ìµœì € RMSE
    regression_models = [p for p in model_performance if p['model_type'] == 'regression']
    if regression_models:
        best_regression = min(regression_models, key=lambda x: x['rmse'])
        print(f"   íšŒê·€ ëª¨ë¸: {best_regression['model_name']} (RMSE: {best_regression['rmse']:.0f}ì²œì›)")

print("="*60)
