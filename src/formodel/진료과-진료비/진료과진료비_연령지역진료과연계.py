# -*- coding: utf-8 -*-
"""
Created on Sat Jun 28 20:43:03 2025
author: jenny

ìƒë³‘ì½”ë“œ/ì§€ì—­ ê¸°ë°˜ + ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° ì—°ê³„
 1) ë¹„ëª¨ìˆ˜ ê²€ì •
 2) ê³ ë¹„ìš© ì—¬ë¶€ ë¶„ë¥˜ ëª¨ë¸
 3) ì§„ë£Œë¹„ íšŒê·€ ëª¨ë¸
 4) LightGBM íšŒê·€ ëª¨ë¸ (ë¡œê·¸ íƒ€ê¹ƒ + CV)
 5) ë¡œê·¸ ìŠ¤ì¼€ì¼ ê¸°ë°˜ ì§„ë£Œë¹„ êµ¬ê°„ ì˜ˆì¸¡
"""

import numpy as np
import pandas as pd
import scipy.stats as stats
import scikit_posthocs as sp
import lightgbm as lgb
import joblib
import os

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier,
    RandomForestRegressor, GradientBoostingRegressor
)
from sklearn.metrics import (
    classification_report,
    mean_absolute_error, mean_squared_error, r2_score
)

# ëª¨ë¸ ì„±ëŠ¥ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
model_performance = []

def calculate_classification_metrics(y_true, y_pred, model_name):
    """ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    
    return {
        'model_name': model_name,
        'model_type': 'classification',
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }

def calculate_regression_metrics(y_true, y_pred, model_name):
    """íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    return {
        'model_name': model_name,
        'model_type': 'regression',
        'mae': mae,
        'rmse': rmse,
        'r2_score': r2
    } 

# ----------------------------------------------------------------------
# 1) ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬
# ----------------------------------------------------------------------
print("=== ë°ì´í„° ë¡œë”© ì‹œì‘ ===")

data_csv = "final_merged_data/ë‹¤ë¹ˆë„ ì§ˆí™˜ í™˜ì ì—°ë ¹ë³„ ë¶„í¬.csv"
mapping_csv = "new_merged_data/df_result2_with_ì‹¬í‰ì›.csv"

# ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° ë¡œë“œ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´)
try:
    df_age_region = pd.read_csv('model_results_ì—°ë ¹ì§€ì—­_ì§„ë£Œê³¼/Stacking_prediction_results_detailed.csv')
    print("âœ… ì—°ë ¹ì§€ì—­ ì§„ë£Œê³¼ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
except:
    print("âš ï¸ ì—°ë ¹ì§€ì—­ ì§„ë£Œê³¼ ë°ì´í„° ì—†ìŒ")
    df_age_region = None

ekqlseh = pd.read_csv(data_csv, encoding="utf-8-sig")
ekqlseh.loc[ekqlseh['êµ¬ë¶„'].str.contains('ì™¸ë˜'), 'ì—°ì¸ì›'] = ekqlseh['ì‹¤ì¸ì›']
ekqlseh = ekqlseh[ekqlseh['êµ¬ë¶„'] != 'ì…ì›(ì‹¤ì¸ì›)']

df = ekqlseh.drop(columns=['ìˆœìœ„', 'ìƒë³‘ëª…', 'ì‹¤ì¸ì›'])
df = df[~df['ì§€ì—­'].isin(['ì„œìš¸', 'ëŒ€ì „', 'ëŒ€êµ¬'])].copy()

# ì¤‘ë³µ í™•ì¸ ë° ì œê±°
print(f"ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: {len(df)}")
print(f"ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")

# ì¤‘ë³µì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•©ë§Œ ìœ ì§€
if df.duplicated().sum() > len(df) * 0.5:  # 50% ì´ìƒ ì¤‘ë³µì´ë©´
    print("ì¤‘ë³µì´ ë„ˆë¬´ ë§ì•„ ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•©ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.")
    # ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ëŠ” ì»¬ëŸ¼ ì‚¬ìš©
    available_keys = ['ìƒë³‘ì½”ë“œ', 'ì§€ì—­', 'êµ¬ë¶„', 'ì—°ì¸ì›', 'ì§„ë£Œë¹„(ì²œì›)', 'ì§„ë£Œê³¼']
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
    existing_keys = [key for key in available_keys if key in df.columns]
    print(f"ì‚¬ìš©í•  ì»¬ëŸ¼: {existing_keys}")
    df = df.drop_duplicates(subset=existing_keys)
    print(f"ê³ ìœ  ì¡°í•© ê¸°ì¤€ ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")
else:
    if df.duplicated().sum() > 0:
        print("ì¼ë°˜ì ì¸ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        df = df.drop_duplicates()
        print(f"ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")

mapping = pd.read_csv(mapping_csv, encoding="utf-8-sig")
df = df.merge(mapping[['ìƒë³‘ì½”ë“œ', 'ì§„ë£Œê³¼']], on='ìƒë³‘ì½”ë“œ', how='left')
df.dropna(subset=['ì§„ë£Œê³¼'], inplace=True)
print(f"ì§„ë£Œê³¼ ë§¤í•‘ í›„ í–‰ ìˆ˜: {len(df)}")

# ----------------------------------------------------------------------
# 2) ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ í”¼ì²˜ ìƒì„±
# ----------------------------------------------------------------------
print("=== ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° ê¸°ë°˜ í”¼ì²˜ ìƒì„± ===")

if df_age_region is not None:
    # ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„°ì—ì„œ ì§„ë£Œê³¼ë³„ í†µê³„ ì •ë³´ ì¶”ì¶œ
    department_stats = df_age_region.groupby('y_actual').agg({
        'top1_probability': 'mean',
        'confidence': 'mean',
        'sample_weight': 'sum',
        'age_num': 'mean',
        'is_major_city': 'mean'
    }).reset_index()
    
    department_stats.columns = ['ì§„ë£Œê³¼', 'í‰ê· í™•ë¥ ', 'í‰ê· ì‹ ë¢°ë„', 'ì´ìƒ˜í”Œìˆ˜', 'í‰ê· ì—°ë ¹', 'ëŒ€ë„ì‹œë¹„ìœ¨']
    
    # ì§„ë£Œê³¼ë³„ë¡œ ë§¤í•‘
    print(f"ì§„ë£Œê³¼ í†µê³„ merge ì „ í–‰ ìˆ˜: {len(df)}")
    print(f"ì§„ë£Œê³¼ í†µê³„ ë°ì´í„° ì§„ë£Œê³¼ ìˆ˜: {len(department_stats)}")
    
    # merge ì „ ì¤‘ë³µ í™•ì¸
    print(f"merge ì „ ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")
    
    df = df.merge(department_stats, on='ì§„ë£Œê³¼', how='left')
    
    # merge í›„ ì¤‘ë³µ í™•ì¸
    print(f"ì§„ë£Œê³¼ í†µê³„ merge í›„ í–‰ ìˆ˜: {len(df)}")
    print(f"merge í›„ ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")
    
    # ì¤‘ë³µì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•©ë§Œ ìœ ì§€
    if df.duplicated().sum() > len(df) * 0.5:  # 50% ì´ìƒ ì¤‘ë³µì´ë©´
        print("ì¤‘ë³µì´ ë„ˆë¬´ ë§ì•„ ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•©ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.")
        # ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        original_keys = ['ìƒë³‘ì½”ë“œ', 'ì§€ì—­', 'ì—°ë ¹ëŒ€', 'êµ¬ë¶„', 'ì—°ì¸ì›', 'ì§„ë£Œë¹„', 'ì§„ë£Œê³¼']
        df = df.drop_duplicates(subset=original_keys)
        print(f"ê³ ìœ  ì¡°í•© ê¸°ì¤€ ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")
    else:
        if df.duplicated().sum() > 0:
            print("ì¼ë°˜ì ì¸ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            df = df.drop_duplicates()
            print(f"ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")
    
    # ì§€ì—­ë³„ í†µê³„ ì •ë³´ ì¶”ì¶œ
    region_stats = df_age_region.groupby('ì§€ì—­').agg({
        'top1_probability': 'mean',
        'confidence': 'mean',
        'sample_weight': 'sum',
        'age_num': 'mean'
    }).reset_index()
    
    region_stats.columns = ['ì§€ì—­', 'ì§€ì—­í‰ê· í™•ë¥ ', 'ì§€ì—­í‰ê· ì‹ ë¢°ë„', 'ì§€ì—­ì´ìƒ˜í”Œìˆ˜', 'ì§€ì—­í‰ê· ì—°ë ¹']
    
    # ì§€ì—­ë³„ë¡œ ë§¤í•‘
    print(f"ì§€ì—­ í†µê³„ merge ì „ í–‰ ìˆ˜: {len(df)}")
    print(f"ì§€ì—­ í†µê³„ ë°ì´í„° ì§€ì—­ ìˆ˜: {len(region_stats)}")
    
    # merge ì „ ì¤‘ë³µ í™•ì¸
    print(f"merge ì „ ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")
    
    df = df.merge(region_stats, on='ì§€ì—­', how='left')
    
    # merge í›„ ì¤‘ë³µ í™•ì¸
    print(f"ì§€ì—­ í†µê³„ merge í›„ í–‰ ìˆ˜: {len(df)}")
    print(f"merge í›„ ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")
    
    # ì¤‘ë³µì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•©ë§Œ ìœ ì§€
    if df.duplicated().sum() > len(df) * 0.5:  # 50% ì´ìƒ ì¤‘ë³µì´ë©´
        print("ì¤‘ë³µì´ ë„ˆë¬´ ë§ì•„ ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•©ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.")
        # ì›ë³¸ ë°ì´í„°ì˜ ê³ ìœ í•œ ì¡°í•© ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        original_keys = ['ìƒë³‘ì½”ë“œ', 'ì§€ì—­', 'ì—°ë ¹ëŒ€', 'êµ¬ë¶„', 'ì—°ì¸ì›', 'ì§„ë£Œë¹„', 'ì§„ë£Œê³¼']
        df = df.drop_duplicates(subset=original_keys)
        print(f"ê³ ìœ  ì¡°í•© ê¸°ì¤€ ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")
    else:
        if df.duplicated().sum() > 0:
            print("ì¼ë°˜ì ì¸ ì¤‘ë³µ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            df = df.drop_duplicates()
            print(f"ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")
    
    # ì—°ë ¹ëŒ€ë³„ í†µê³„ ì •ë³´ ì¶”ì¶œ
    age_stats = df_age_region.groupby('age_group').agg({
        'top1_probability': 'mean',
        'confidence': 'mean',
        'sample_weight': 'sum'
    }).reset_index()
    
    age_stats.columns = ['ì—°ë ¹ëŒ€', 'ì—°ë ¹ëŒ€í‰ê· í™•ë¥ ', 'ì—°ë ¹ëŒ€í‰ê· ì‹ ë¢°ë„', 'ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜']
    
    # ì—°ë ¹ëŒ€ ì •ë³´ê°€ ìˆë‹¤ë©´ ë§¤í•‘ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    if 'age_group' in df.columns:
        df = df.merge(age_stats, on='ì—°ë ¹ëŒ€', how='left')
    else:
        # ì—°ë ¹ëŒ€ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì „ì²´ í‰ê· ê°’ ì‚¬ìš©
        df['ì—°ë ¹ëŒ€í‰ê· í™•ë¥ '] = age_stats['ì—°ë ¹ëŒ€í‰ê· í™•ë¥ '].mean()
        df['ì—°ë ¹ëŒ€í‰ê· ì‹ ë¢°ë„'] = age_stats['ì—°ë ¹ëŒ€í‰ê· ì‹ ë¢°ë„'].mean()
        df['ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜'] = age_stats['ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜'].mean()
    
    # ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±
    df['ì§„ë£Œê³¼_ì§€ì—­_ìƒí˜¸ì‘ìš©'] = df['í‰ê· í™•ë¥ '] * df['ì§€ì—­í‰ê· í™•ë¥ ']
    df['ì§„ë£Œê³¼_ì—°ë ¹ëŒ€_ìƒí˜¸ì‘ìš©'] = df['í‰ê· í™•ë¥ '] * df['ì—°ë ¹ëŒ€í‰ê· í™•ë¥ ']
    df['ì§€ì—­_ì—°ë ¹ëŒ€_ìƒí˜¸ì‘ìš©'] = df['ì§€ì—­í‰ê· í™•ë¥ '] * df['ì—°ë ¹ëŒ€í‰ê· í™•ë¥ ']
    
    # ë³µí•© ì‹ ë¢°ë„ ì§€í‘œ
    df['ì¢…í•©ì‹ ë¢°ë„'] = (df['í‰ê· ì‹ ë¢°ë„'] + df['ì§€ì—­í‰ê· ì‹ ë¢°ë„'] + df['ì—°ë ¹ëŒ€í‰ê· ì‹ ë¢°ë„']) / 3
    
    # ë¡œê·¸ ë³€í™˜
    df['ì´ìƒ˜í”Œìˆ˜_log'] = np.log1p(df['ì´ìƒ˜í”Œìˆ˜'])
    df['ì§€ì—­ì´ìƒ˜í”Œìˆ˜_log'] = np.log1p(df['ì§€ì—­ì´ìƒ˜í”Œìˆ˜'])
    df['ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜_log'] = np.log1p(df['ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜'])
    
    # NaN ê°’ ì²˜ë¦¬
    df = df.fillna(0)
    
    print(f"ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ í”¼ì²˜ ìƒì„± ì™„ë£Œ")
    print(f"ì¶”ê°€ëœ í”¼ì²˜ ìˆ˜: {len(['í‰ê· í™•ë¥ ', 'í‰ê· ì‹ ë¢°ë„', 'ì´ìƒ˜í”Œìˆ˜', 'í‰ê· ì—°ë ¹', 'ëŒ€ë„ì‹œë¹„ìœ¨', 'ì§€ì—­í‰ê· í™•ë¥ ', 'ì§€ì—­í‰ê· ì‹ ë¢°ë„', 'ì§€ì—­ì´ìƒ˜í”Œìˆ˜', 'ì§€ì—­í‰ê· ì—°ë ¹', 'ì—°ë ¹ëŒ€í‰ê· í™•ë¥ ', 'ì—°ë ¹ëŒ€í‰ê· ì‹ ë¢°ë„', 'ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜', 'ì§„ë£Œê³¼_ì§€ì—­_ìƒí˜¸ì‘ìš©', 'ì§„ë£Œê³¼_ì—°ë ¹ëŒ€_ìƒí˜¸ì‘ìš©', 'ì§€ì—­_ì—°ë ¹ëŒ€_ìƒí˜¸ì‘ìš©', 'ì¢…í•©ì‹ ë¢°ë„', 'ì´ìƒ˜í”Œìˆ˜_log', 'ì§€ì—­ì´ìƒ˜í”Œìˆ˜_log', 'ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜_log'])}ê°œ")
else:
    # ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
    df['í‰ê· í™•ë¥ '] = 0
    df['í‰ê· ì‹ ë¢°ë„'] = 0
    df['ì´ìƒ˜í”Œìˆ˜'] = 0
    df['í‰ê· ì—°ë ¹'] = 0
    df['ëŒ€ë„ì‹œë¹„ìœ¨'] = 0
    df['ì§€ì—­í‰ê· í™•ë¥ '] = 0
    df['ì§€ì—­í‰ê· ì‹ ë¢°ë„'] = 0
    df['ì§€ì—­ì´ìƒ˜í”Œìˆ˜'] = 0
    df['ì§€ì—­í‰ê· ì—°ë ¹'] = 0
    df['ì—°ë ¹ëŒ€í‰ê· í™•ë¥ '] = 0
    df['ì—°ë ¹ëŒ€í‰ê· ì‹ ë¢°ë„'] = 0
    df['ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜'] = 0
    df['ì§„ë£Œê³¼_ì§€ì—­_ìƒí˜¸ì‘ìš©'] = 0
    df['ì§„ë£Œê³¼_ì—°ë ¹ëŒ€_ìƒí˜¸ì‘ìš©'] = 0
    df['ì§€ì—­_ì—°ë ¹ëŒ€_ìƒí˜¸ì‘ìš©'] = 0
    df['ì¢…í•©ì‹ ë¢°ë„'] = 0
    df['ì´ìƒ˜í”Œìˆ˜_log'] = 0
    df['ì§€ì—­ì´ìƒ˜í”Œìˆ˜_log'] = 0
    df['ì—°ë ¹ëŒ€ì´ìƒ˜í”Œìˆ˜_log'] = 0

# ----------------------------------------------------------------------
# 3) ë¹„ëª¨ìˆ˜ ê²€ì •: Kruskalâ€“Wallis + Dunn's
# ----------------------------------------------------------------------
groups = [g['ì§„ë£Œë¹„(ì²œì›)'].values for _, g in df.groupby('ìƒë³‘ì½”ë“œ') if len(g) >= 3]
H, p = stats.kruskal(*groups)
print(f"=== Kruskalâ€“Wallis ê²€ì •: H={H:.4f}, p-value={p:.4e} ===")

dunn = sp.posthoc_dunn(df, val_col='ì§„ë£Œë¹„(ì²œì›)', group_col='ìƒë³‘ì½”ë“œ', p_adjust='bonferroni')
print("=== Dunn's post-hoc (Bonferroni) ===")
print(dunn)

# ----------------------------------------------------------------------
# 4) ë¶„ë¥˜ ëª¨ë¸: ê³ ë¹„ìš© ì—¬ë¶€ ì˜ˆì¸¡ (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨)
# ----------------------------------------------------------------------
thr = df['ì§„ë£Œë¹„(ì²œì›)'].quantile(0.75)
df['high_cost'] = (df['ì§„ë£Œë¹„(ì²œì›)'] >= thr).astype(int)

# Decision Tree (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨)
X_dt = pd.get_dummies(df[['ìƒë³‘ì½”ë“œ', 'í‰ê· í™•ë¥ ', 'ì¢…í•©ì‹ ë¢°ë„']], prefix='', prefix_sep='')
y = df['high_cost']
X_tr_dt, X_te_dt, y_tr, y_te = train_test_split(
    X_dt, y, test_size=0.3, random_state=42, stratify=y
)
dt = DecisionTreeClassifier(max_depth=4, class_weight='balanced', random_state=42)
dt.fit(X_tr_dt, y_tr)
y_pred_dt = dt.predict(X_te_dt)
print("\n=== DecisionTreeClassifier (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨) ===")
print(classification_report(y_te, y_pred_dt))

# ì„±ëŠ¥ ì €ì¥
dt_performance = calculate_classification_metrics(y_te, y_pred_dt, "DecisionTree_Classification")
model_performance.append(dt_performance)

# RandomForest & GradientBoosting (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨)
X_rf = pd.get_dummies(df[['ìƒë³‘ì½”ë“œ', 'ì§€ì—­', 'í‰ê· í™•ë¥ ', 'ì¢…í•©ì‹ ë¢°ë„', 'ì§„ë£Œê³¼_ì§€ì—­_ìƒí˜¸ì‘ìš©', 'ì´ìƒ˜í”Œìˆ˜_log']], dtype=int)
X_tr_rf, X_te_rf, _, _ = train_test_split(
    X_rf, y, test_size=0.3, random_state=42, stratify=y
)
rf = RandomForestClassifier(
    n_estimators=200, max_depth=6,
    class_weight='balanced', random_state=42, n_jobs=-1
)
gb = GradientBoostingClassifier(
    n_estimators=200, learning_rate=0.05,
    max_depth=4, random_state=42
)
rf.fit(X_tr_rf, y_tr)
gb.fit(X_tr_rf, y_tr)
y_pred_rf = rf.predict(X_te_rf)
y_pred_gb = gb.predict(X_te_rf)
print("\n=== RandomForestClassifier (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨) ===")
print(classification_report(y_te, y_pred_rf))
print("\n=== GradientBoostingClassifier (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨) ===")
print(classification_report(y_te, y_pred_gb))

# ì„±ëŠ¥ ì €ì¥
rf_performance = calculate_classification_metrics(y_te, y_pred_rf, "RandomForest_Classification")
gb_performance = calculate_classification_metrics(y_te, y_pred_gb, "GradientBoosting_Classification")
model_performance.extend([rf_performance, gb_performance])

# ----------------------------------------------------------------------
# 5) íšŒê·€ ëª¨ë¸: ì§„ë£Œë¹„ ì§ì ‘ ì˜ˆì¸¡ (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨)
# ----------------------------------------------------------------------
X_reg = X_rf.copy()
y_reg = df['ì§„ë£Œë¹„(ì²œì›)'].values
X_tr_rg, X_te_rg, y_tr_rg, y_te_rg = train_test_split(
    X_reg, y_reg, test_size=0.3, random_state=42
)

dtr = DecisionTreeRegressor(max_depth=6, random_state=42)
rfr = RandomForestRegressor(n_estimators=200, max_depth=8, random_state=42, n_jobs=-1)
gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)
for m in (dtr, rfr, gbr):
    m.fit(X_tr_rg, y_tr_rg)
print("\n=== íšŒê·€ ëª¨ë¸ í‰ê°€ (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨) ===")
for name, m in [("DT", dtr), ("RF", rfr), ("GB", gbr)]:
    pred = m.predict(X_te_rg)
    print(f"{name} â†’ MAE: {mean_absolute_error(y_te_rg, pred):.0f}ì²œì›, RMSE: {np.sqrt(mean_squared_error(y_te_rg, pred)):.0f}ì²œì›")
    
    # ì„±ëŠ¥ ì €ì¥
    reg_performance = calculate_regression_metrics(y_te_rg, pred, f"{name}_Regression")
    model_performance.append(reg_performance)

# ----------------------------------------------------------------------
# 6) ë¡œê·¸ ìŠ¤ì¼€ì¼ ê¸°ë°˜ ì§„ë£Œë¹„ êµ¬ê°„ ì˜ˆì¸¡ (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨)
# ----------------------------------------------------------------------
# 6.1) ë¡œê·¸ ìŠ¤ì¼€ì¼ êµ¬ê°„ ì •ì˜
min_v = df['ì§„ë£Œë¹„(ì²œì›)'].min()
max_v = df['ì§„ë£Œë¹„(ì²œì›)'].max()
bins = np.logspace(np.log10(min_v), np.log10(max_v), num=6)
# 6.2) êµ¬ê°„ í´ë˜ìŠ¤ í• ë‹¹
labels = pd.cut(df['ì§„ë£Œë¹„(ì²œì›)'], bins=bins, labels=False, include_lowest=True)
# 6.3) NaN & í¬ê·€ êµ¬ê°„ ì œê±°
valid_idx = labels.dropna().index
counts = labels.loc[valid_idx].value_counts().sort_index()
rare = counts[counts < 2].index
use_idx = valid_idx.difference(labels[labels.isin(rare)].index)
X_clean = X_reg.loc[use_idx]
y_clean = labels.loc[use_idx]
# 6.4) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_tr, X_te, y_tr, y_te = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42, stratify=y_clean)
# 6.5) ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥
lgb_clf = lgb.LGBMClassifier(objective='multiclass', num_class=len(y_clean.unique()), learning_rate=0.05, n_estimators=200, num_leaves=31, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5, verbosity=-1, seed=42)
lgb_clf.fit(X_tr, y_tr)
y_pred = lgb_clf.predict(X_te)
print("\n=== ë¡œê·¸ ìŠ¤ì¼€ì¼ êµ¬ê°„ ë¶„ë¥˜ ì„±ëŠ¥ (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨) ===")
print(classification_report(y_te, y_pred))

# LightGBM ì„±ëŠ¥ ì €ì¥
lgb_performance = calculate_classification_metrics(y_te, y_pred, "LightGBM_Classification")
model_performance.append(lgb_performance)

# 6.6) ëŒ€í‘œ ì§„ë£Œë¹„ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_cost_bin(code, region, model, feat_cols, bins, age_region_features=None):
    """
    ìƒë³‘ì½”ë“œì™€ ì§€ì—­ìœ¼ë¡œ ë¡œê·¸ ìŠ¤ì¼€ì¼ êµ¬ê°„ í´ë˜ìŠ¤ì™€
    ëŒ€í‘œ ì§„ë£Œë¹„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ (ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„° í¬í•¨)
    """
    # ì…ë ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì›-í•« ì¸ì½”ë”©
    df_in = pd.DataFrame([{'ìƒë³‘ì½”ë“œ': code, 'ì§€ì—­': region}])
    
    # ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if age_region_features is not None:
        for key, value in age_region_features.items():
            df_in[key] = value
    
    X_in = pd.get_dummies(df_in, columns=['ìƒë³‘ì½”ë“œ', 'ì§€ì—­'], dtype=int)
    X_in = X_in.reindex(columns=feat_cols, fill_value=0)

    # í´ë˜ìŠ¤ ì˜ˆì¸¡
    bin_pred_raw = model.predict(X_in)[0]
    bin_idx = int(bin_pred_raw)

    # í´ë˜ìŠ¤ë³„ ëŒ€í‘œê°’ ì¶”ì¶œ (ì¤‘ì•™ê°’)
    midpoint = (bins[bin_idx] + bins[bin_idx + 1]) / 2
    return bin_idx, midpoint

# ì˜ˆì‹œ ì‚¬ìš©:
feat_cols = X_reg.columns.tolist()
example_code, example_region = 'M48', 'ë¶€ì‚°'
# ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ ì˜ˆì¸¡ê°’ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” í•´ë‹¹ ìƒë³‘ì½”ë“œì˜ í†µê³„ê°’ì„ ì‚¬ìš©)
age_region_features = {
    'í‰ê· í™•ë¥ ': 0.8,
    'ì¢…í•©ì‹ ë¢°ë„': 0.75,
    'ì§„ë£Œê³¼_ì§€ì—­_ìƒí˜¸ì‘ìš©': 0.6,
    'ì´ìƒ˜í”Œìˆ˜_log': 8.5
}
bin_label, est_cost = predict_cost_bin(
    example_code, example_region,
    lgb_clf, feat_cols, bins, age_region_features
)
print(f"ì˜ˆì¸¡ êµ¬ê°„: {bin_label}, ëŒ€í‘œ ì§„ë£Œë¹„: {est_cost:.0f}ì²œì›")

# ----------------------------------------------------------------------
# 7) ê²°ê³¼ ì €ì¥
# ----------------------------------------------------------------------
print("\n=== ê²°ê³¼ ì €ì¥ ì‹œì‘ ===")

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
results_dir = "model_results_ì§„ë£Œê³¼ì§„ë£Œë¹„_ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼"
os.makedirs(results_dir, exist_ok=True)

# ëª¨ë¸ ì €ì¥
joblib.dump(dt, f"{results_dir}/dt_highcost_model_age_region.pkl")
joblib.dump(rf, f"{results_dir}/rf_highcost_model_age_region.pkl")
joblib.dump(gb, f"{results_dir}/gb_highcost_model_age_region.pkl")
joblib.dump(dtr, f"{results_dir}/dtr_cost_regressor_age_region.pkl")
joblib.dump(rfr, f"{results_dir}/rfr_cost_regressor_age_region.pkl")
joblib.dump(gbr, f"{results_dir}/gbr_cost_regressor_age_region.pkl")
joblib.dump(lgb_clf, f"{results_dir}/lgb_cost_bin_classifier_age_region.pkl")

# ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡
preds = []
for _, row in df.iterrows():
    # í•´ë‹¹ ìƒë³‘ì½”ë“œì˜ ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ í†µê³„ê°’ ê°€ì ¸ì˜¤ê¸°
    age_region_features = {
        'í‰ê· í™•ë¥ ': row.get('í‰ê· í™•ë¥ ', 0),
        'ì¢…í•©ì‹ ë¢°ë„': row.get('ì¢…í•©ì‹ ë¢°ë„', 0),
        'ì§„ë£Œê³¼_ì§€ì—­_ìƒí˜¸ì‘ìš©': row.get('ì§„ë£Œê³¼_ì§€ì—­_ìƒí˜¸ì‘ìš©', 0),
        'ì´ìƒ˜í”Œìˆ˜_log': row.get('ì´ìƒ˜í”Œìˆ˜_log', 0)
    }
    
    try:
        bin_label, est_cost = predict_cost_bin(
            row['ìƒë³‘ì½”ë“œ'], row['ì§€ì—­'],
            lgb_clf, feat_cols, bins, age_region_features
        )
        preds.append((bin_label, est_cost))
    except:
        preds.append((0, 0))

# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ dfì— ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
df['pred_bin_age_region'], df['pred_cost_age_region'] = zip(*preds)

# ìµœì¢… ì¤‘ë³µ í™•ì¸ ë° ì œê±°
print(f"ì˜ˆì¸¡ ì™„ë£Œ í›„ ë°ì´í„° í–‰ ìˆ˜: {len(df)}")
print(f"ìµœì¢… ì¤‘ë³µ í–‰ ìˆ˜: {df.duplicated().sum()}")
if df.duplicated().sum() > 0:
    print("ìµœì¢… ì¤‘ë³µ í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.")
    df = df.drop_duplicates()
    print(f"ìµœì¢… ì¤‘ë³µ ì œê±° í›„ í–‰ ìˆ˜: {len(df)}")

# CSVë¡œ ì €ì¥
output_path = f"{results_dir}/ì§„ë£Œë¹„_êµ¬ê°„ì˜ˆì¸¡ê²°ê³¼_ì—°ë ¹ì§€ì—­ì§„ë£Œê³¼ì—°ê³„.csv"
df.to_csv(output_path, index=False, encoding='utf-8-sig')

# ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ë° CSV ì €ì¥
print("\n" + "="*60)
print("ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½")
print("="*60)

# ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½
classification_models = [p for p in model_performance if p['model_type'] == 'classification']
if classification_models:
    print("\nğŸ“Š ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥:")
    print("-" * 50)
    for model in classification_models:
        print(f"{model['model_name']}:")
        print(f"  ì •í™•ë„: {model['accuracy']:.4f}")
        print(f"  ì •ë°€ë„: {model['precision']:.4f}")
        print(f"  ì¬í˜„ìœ¨: {model['recall']:.4f}")
        print(f"  F1ì ìˆ˜: {model['f1_score']:.4f}")
        print()

# íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½
regression_models = [p for p in model_performance if p['model_type'] == 'regression']
if regression_models:
    print("\nğŸ“ˆ íšŒê·€ ëª¨ë¸ ì„±ëŠ¥:")
    print("-" * 50)
    for model in regression_models:
        print(f"{model['model_name']}:")
        print(f"  MAE: {model['mae']:.0f}ì²œì›")
        print(f"  RMSE: {model['rmse']:.0f}ì²œì›")
        print(f"  RÂ²: {model['r2_score']:.4f}")
        print()

# ì„±ëŠ¥ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CSV ì €ì¥
performance_df = pd.DataFrame(model_performance)
performance_csv_path = f"{results_dir}/ëª¨ë¸ë³„_ì„±ëŠ¥_ìš”ì•½.csv"
performance_df.to_csv(performance_csv_path, index=False, encoding='utf-8-sig')

print(f"ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ì´ '{performance_csv_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ì˜ˆì¸¡ ê²°ê³¼ë¥¼ '{output_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
print(f"ëª¨ë“  ê²°ê³¼ê°€ '{results_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")