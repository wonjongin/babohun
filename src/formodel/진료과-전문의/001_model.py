import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict, TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_curve, auc, roc_auc_score
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.decomposition import PCA
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import StandardScaler, RobustScaler
from scipy import stats

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.linear_model import LinearRegression

from optuna import create_study

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df_pred = pd.read_csv('analysis_data/ë³‘ì›ë³„_ì§„ë£Œê³¼ë³„_ì…ì›ì™¸ë˜_í†µí•©_ì‹œê³„ì—´ì˜ˆì¸¡ê²°ê³¼_ê°œì„ .csv')
df_info = pd.read_csv('new_merged_data/ë³‘ì›_í†µí•©_ë°ì´í„°.csv')

# 2. ìµœê·¼ ì—°ë„(2023)ë§Œ ì‚¬ìš©
df_pred = df_pred[df_pred['ì—°ë„'] == 2023]

# 3. ë³‘ì›ëª… ì»¬ëŸ¼ëª… í†µì¼ ë° ì „ì²˜ë¦¬
df_pred['ë³‘ì›ëª…'] = df_pred['ë³‘ì›'].astype(str).str.strip()
df_pred['ì§„ë£Œê³¼'] = df_pred['ì§„ë£Œê³¼'].astype(str).str.strip()
df_pred['ì—°ë„'] = df_pred['ì—°ë„'].astype(str).str.strip()
df_info['ë³‘ì›ëª…'] = df_info['ë³‘ì›ëª…'].astype(str).str.strip()

# 4. ì§„ë£Œê³¼ë³„ ì „ë¬¸ì˜ìˆ˜ ì»¬ëŸ¼ëª… ì¶”ì¶œ
def get_doc_col(ì§„ë£Œê³¼):
    return f"{ì§„ë£Œê³¼}_ì „ë¬¸ì˜ìˆ˜"

# 5. merge ë° X, y ìƒì„± (ê°œì„ ëœ ë²„ì „)
X_rows = []
y_list = []

for idx, row in df_pred.iterrows():
    ë³‘ì› = row['ë³‘ì›ëª…']
    ì§„ë£Œê³¼ = row['ì§„ë£Œê³¼']
    ì—°ë„ = row['ì—°ë„']
    
    # ëª¨ë“  ì˜ˆì¸¡ê°’ í™œìš©
    arima_pred = row['ARIMAì˜ˆì¸¡']
    rf_pred = row['RFì˜ˆì¸¡']
    xgb_pred = row['XGBì˜ˆì¸¡']
    ì‹¤ì œê°’ = row['ì‹¤ì œê°’']
    
    info_row = df_info[df_info['ë³‘ì›ëª…'] == ë³‘ì›]
    doc_col = get_doc_col(ì§„ë£Œê³¼)
    
    if len(info_row) == 0:
        continue
    if doc_col in info_row.columns:
        y_val = info_row.iloc[0][doc_col]
        if pd.notnull(y_val):
            # ê¸°ë³¸ ì •ë³´
            row_data = {
                'ARIMAì˜ˆì¸¡': arima_pred,
                'RFì˜ˆì¸¡': rf_pred, 
                'XGBì˜ˆì¸¡': xgb_pred,
                'ì‹¤ì œê°’': ì‹¤ì œê°’,
                'ë³‘ì›ëª…': ë³‘ì›, 
                'ì§„ë£Œê³¼': ì§„ë£Œê³¼, 
                'ì—°ë„': ì—°ë„
            }
            
            # ë³‘ìƒìˆ˜ ì •ë³´ ì¶”ê°€
            bed_columns = [
                'ê°•ë‚´ì¹˜ë£Œì‹¤', 'ê²©ë¦¬ë³‘ì‹¤', 'ë¬´ê· ì¹˜ë£Œì‹¤', 'ë¬¼ë¦¬ì¹˜ë£Œì‹¤', 'ë°©ì‚¬ì„ ì˜¥ì†Œ', 'ë¶„ë§Œì‹¤', 'ìˆ˜ìˆ ì‹¤', 'ì‹ ìƒì•„ì‹¤', 
                'ì‘ê¸‰ì‹¤', 'ì¸ê³µì‹ ì¥ì‹¤', 'ì¼ë°˜ì…ì›ì‹¤_ìƒê¸‰', 'ì¼ë°˜ì…ì›ì‹¤_ì¼ë°˜', 'ì •ì‹ ê³¼ê°œë°©_ìƒê¸‰', 'ì •ì‹ ê³¼ê°œë°©_ì¼ë°˜', 
                'ì •ì‹ ê³¼íì‡„_ìƒê¸‰', 'ì •ì‹ ê³¼íì‡„_ì¼ë°˜', 'ì¤‘í™˜ìì‹¤_ì„±ì¸', 'ì¤‘í™˜ìì‹¤_ì†Œì•„', 'ì¤‘í™˜ìì‹¤_ì‹ ìƒì•„', 'íšŒë³µì‹¤',
                'ê°€ì¡±ì‹¤', 'ê°„í˜¸ì‚¬ì‹¤', 'ëª©ìš•ì‹¤', 'ìƒë‹´ì‹¤', 'ì„ì¢…ì‹¤', 'ì²˜ì¹˜ì‹¤', 'í™”ì¥ì‹¤'
            ]
            
            for bed_col in bed_columns:
                if bed_col in info_row.columns:
                    bed_val = info_row.iloc[0][bed_col]
                    row_data[bed_col] = bed_val if pd.notnull(bed_val) else 0
                else:
                    row_data[bed_col] = 0
            
            # ì´ ë³‘ìƒìˆ˜ ê³„ì‚°
            total_beds = sum([row_data[col] for col in bed_columns if col in row_data])
            row_data['ì´ë³‘ìƒìˆ˜'] = total_beds
            
            X_rows.append(row_data)
            y_list.append(y_val)

X = pd.DataFrame(X_rows)
y = pd.Series(y_list, name='ì „ë¬¸ì˜ìˆ˜')

# 6. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¶”ê°€
print("=== í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘ ===")

# ì˜ˆì¸¡ê°’ë“¤ì˜ í†µê³„ì  íŠ¹ì„±
X['ì˜ˆì¸¡ê°’_í‰ê· '] = X[['ARIMAì˜ˆì¸¡', 'RFì˜ˆì¸¡', 'XGBì˜ˆì¸¡']].mean(axis=1)
X['ì˜ˆì¸¡ê°’_í‘œì¤€í¸ì°¨'] = X[['ARIMAì˜ˆì¸¡', 'RFì˜ˆì¸¡', 'XGBì˜ˆì¸¡']].std(axis=1)
X['ì˜ˆì¸¡ê°’_ìµœëŒ€'] = X[['ARIMAì˜ˆì¸¡', 'RFì˜ˆì¸¡', 'XGBì˜ˆì¸¡']].max(axis=1)
X['ì˜ˆì¸¡ê°’_ìµœì†Œ'] = X[['ARIMAì˜ˆì¸¡', 'RFì˜ˆì¸¡', 'XGBì˜ˆì¸¡']].min(axis=1)

# ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ (ì˜ˆì¸¡ ì •í™•ë„ ì§€í‘œ)
X['ARIMA_ì˜¤ì°¨'] = abs(X['ARIMAì˜ˆì¸¡'] - X['ì‹¤ì œê°’'])
X['RF_ì˜¤ì°¨'] = abs(X['RFì˜ˆì¸¡'] - X['ì‹¤ì œê°’'])
X['XGB_ì˜¤ì°¨'] = abs(X['XGBì˜ˆì¸¡'] - X['ì‹¤ì œê°’'])

# ì˜ˆì¸¡ê°’ë“¤ì˜ ê°€ì¤‘ í‰ê·  (ì„±ëŠ¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜)
# CSVì—ì„œ RÂ² ê°’ì„ ë³´ë©´ XGBê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
X['ê°€ì¤‘ì˜ˆì¸¡ê°’'] = (0.2 * X['ARIMAì˜ˆì¸¡'] + 0.3 * X['RFì˜ˆì¸¡'] + 0.5 * X['XGBì˜ˆì¸¡'])

# ë¡œê·¸ ë³€í™˜ (í° ê°’ë“¤ì˜ ì˜í–¥ ì¤„ì´ê¸°) - ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
X['ARIMAì˜ˆì¸¡_log'] = np.log1p(np.abs(X['ARIMAì˜ˆì¸¡']))
X['RFì˜ˆì¸¡_log'] = np.log1p(np.abs(X['RFì˜ˆì¸¡']))
X['XGBì˜ˆì¸¡_log'] = np.log1p(np.abs(X['XGBì˜ˆì¸¡']))
X['ì‹¤ì œê°’_log'] = np.log1p(np.abs(X['ì‹¤ì œê°’']))

# ë¹„ìœ¨ í”¼ì²˜ - 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
X['ARIMA_ë¹„ìœ¨'] = np.where(X['ì‹¤ì œê°’'] != 0, X['ARIMAì˜ˆì¸¡'] / X['ì‹¤ì œê°’'], 1.0)
X['RF_ë¹„ìœ¨'] = np.where(X['ì‹¤ì œê°’'] != 0, X['RFì˜ˆì¸¡'] / X['ì‹¤ì œê°’'], 1.0)
X['XGB_ë¹„ìœ¨'] = np.where(X['ì‹¤ì œê°’'] != 0, X['XGBì˜ˆì¸¡'] / X['ì‹¤ì œê°’'], 1.0)

# ë³‘ìƒìˆ˜ ê´€ë ¨ ë¹„ìœ¨
X['ë³‘ìƒë‹¹ì˜ˆì¸¡í™˜ììˆ˜'] = X['ê°€ì¤‘ì˜ˆì¸¡ê°’'] / (X['ì´ë³‘ìƒìˆ˜'] + 1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€

print(f"ê¸°ì¡´ í”¼ì²˜ ìˆ˜: 3ê°œ (ARIMA, RF, XGB)")
print(f"ìƒˆë¡œìš´ í”¼ì²˜ ìˆ˜: {X.shape[1] - 3}ê°œ")
print(f"ì´ í”¼ì²˜ ìˆ˜: {X.shape[1]}ê°œ")

# 7. ë³‘ì›ëª…, ì§„ë£Œê³¼, ì—°ë„ ì›í•« ì¸ì½”ë”©
X = pd.get_dummies(X, columns=['ë³‘ì›ëª…', 'ì§„ë£Œê³¼', 'ì—°ë„'])

print(f"ì›í•« ì¸ì½”ë”© í›„ ì´ í”¼ì²˜ ìˆ˜: {X.shape[1]}ê°œ")

# 8. ë°ì´í„° ì •ì œ (NaN, ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬)
print("=== ë°ì´í„° ì •ì œ ì‹œì‘ ===")

# NaN ê°’ í™•ì¸
print(f"NaN ê°’ ê°œìˆ˜: {X.isna().sum().sum()}")
print(f"ë¬´í•œëŒ€ ê°’ ê°œìˆ˜: {np.isinf(X.select_dtypes(include=[np.number])).sum().sum()}")

# NaN ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
X = X.fillna(0)

# ë¬´í•œëŒ€ ê°’ì„ í° ê°’ìœ¼ë¡œ ëŒ€ì²´
X = X.replace([np.inf, -np.inf], 0)

# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ì¶”ê°€ ì •ì œ
numeric_columns = X.select_dtypes(include=[np.number]).columns
for col in numeric_columns:
    # ì´ìƒì¹˜ ì œê±° (99% ë¶„ìœ„ìˆ˜ ì´ìƒ)
    q99 = X[col].quantile(0.99)
    X[col] = np.where(X[col] > q99, q99, X[col])

    # ìŒìˆ˜ ê°’ ì²˜ë¦¬ (ë¡œê·¸ ë³€í™˜ëœ ì»¬ëŸ¼ ì œì™¸)
    if 'log' not in col and 'ë¹„ìœ¨' not in col:
        X[col] = np.abs(X[col])

print("=== ë°ì´í„° ì •ì œ ì™„ë£Œ ===")
print("=== í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ ===")
print()

# 9. train/test ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 10. í•™ìŠµ ë° í‰ê°€ (ê°œì„ ëœ ë²„ì „)
import numpy as np

print("=== ê°œì„ ëœ ëª¨ë¸ë§ ì‹œì‘ ===")
print(f"ë°ì´í„° í¬ê¸°: {X.shape}")
print(f"íƒ€ê²Ÿ ë¶„í¬: {y.describe()}")

# 1) ë°ì´í„° ì „ì²˜ë¦¬ ê°•í™”
scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•í•œ ìŠ¤ì¼€ì¼ë§

# 2) ì´ìƒì¹˜ ì œê±°
z_scores = stats.zscore(y)
outliers = (abs(z_scores) > 3)
print(f"ì´ìƒì¹˜ ì œê±° ì „: {len(y)}ê°œ")
print(f"ì´ìƒì¹˜ ì œê±° í›„: {len(y[~outliers])}ê°œ")

# ì´ìƒì¹˜ê°€ ë§ì§€ ì•Šìœ¼ë©´ ì œê±°í•˜ì§€ ì•ŠìŒ
if outliers.sum() < len(y) * 0.1:  # 10% ë¯¸ë§Œì´ë©´ ì œê±°í•˜ì§€ ì•ŠìŒ
    print("ì´ìƒì¹˜ê°€ ì ì–´ ì œê±°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    y_clean = y
    X_clean = X
else:
    y_clean = y[~outliers]
    X_clean = X[~outliers]
    print("ì´ìƒì¹˜ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.")

# 3) í”¼ì²˜ ì„ íƒ ì¶”ê°€
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import RandomForestRegressor

print("=== í”¼ì²˜ ì„ íƒ ì‹œì‘ ===")

# ìƒê´€ê´€ê³„ê°€ ë†’ì€ í”¼ì²˜ ì œê±°
correlation_matrix = X_clean.corr()
high_corr_features = np.where(np.abs(correlation_matrix) > 0.95)
high_corr_features = [(correlation_matrix.index[x], correlation_matrix.columns[y]) 
                      for x, y in zip(*high_corr_features) if x != y and x < y]

if high_corr_features:
    print(f"ë†’ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ì œê±°: {len(high_corr_features)}ìŒ")
    # ì²« ë²ˆì§¸ í”¼ì²˜ë§Œ ìœ ì§€
    features_to_drop = [pair[1] for pair in high_corr_features]
    X_clean = X_clean.drop(columns=features_to_drop)
    print(f"ì œê±° í›„ í”¼ì²˜ ìˆ˜: {X_clean.shape[1]}")

# 4) ê°œì„ ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ë” ì•ˆì „í•œ ì„¤ì •)
models_improved = {
    'XGBoost': XGBRegressor(random_state=42, n_estimators=100, max_depth=3),
    'LightGBM': LGBMRegressor(verbose=-1, random_state=42, n_estimators=100, max_depth=3),
    'CatBoost': CatBoostRegressor(verbose=0, random_state=42, iterations=100, depth=3),
    'RandomForest': RandomForestRegressor(random_state=42, n_estimators=100, max_depth=3),
    'Ridge': Ridge(random_state=42, alpha=1.0),
    'Lasso': Lasso(random_state=42, alpha=1.0)
}

# 5) êµì°¨ê²€ì¦ ë° í‰ê°€ (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)
cv = KFold(n_splits=3, shuffle=True, random_state=42)  # 5ì—ì„œ 3ìœ¼ë¡œ ì¤„ì„

print("=== ê°œì„ ëœ ëª¨ë¸ í‰ê°€ ì‹œì‘ ===")
for name, model in models_improved.items():
    print(f"\n--- {name} ---")
    
    try:
        # êµì°¨ê²€ì¦
        rmse_scores = np.sqrt(-cross_val_score(model, X_clean, y_clean, cv=cv, scoring='neg_mean_squared_error'))
        mae_scores = -cross_val_score(model, X_clean, y_clean, cv=cv, scoring='neg_mean_absolute_error')
        r2_scores = cross_val_score(model, X_clean, y_clean, cv=cv, scoring='r2')
        
        print(f'  CV RMSE: {rmse_scores.mean():.2f} Â± {rmse_scores.std():.2f}')
        print(f'  CV MAE:  {mae_scores.mean():.2f} Â± {mae_scores.std():.2f}')
        print(f'  CV R2:   {r2_scores.mean():.3f} Â± {r2_scores.std():.3f}')
    except Exception as e:
        print(f'  ì˜¤ë¥˜ ë°œìƒ: {str(e)}')
        continue

# 6) ì•™ìƒë¸” ëª¨ë¸ (ê°œì„ ëœ ë²„ì „)
print("\n=== ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ===")

# ê°œë³„ ëª¨ë¸ í•™ìŠµ
trained_models = {}
for name, model in models_improved.items():
    try:
        model.fit(X_clean, y_clean)
        trained_models[name] = model
        print(f"âœ… {name} í•™ìŠµ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ {name} í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
        continue

if len(trained_models) == 0:
    print("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    # ê¸°ë³¸ ëª¨ë¸ë¡œ ëŒ€ì²´
    basic_model = RandomForestRegressor(random_state=42, n_estimators=100)
    basic_model.fit(X_clean, y_clean)
    trained_models['BasicRF'] = basic_model

# ê°€ì¤‘ ì•™ìƒë¸”
def weighted_ensemble_predict(X, models, weights):
    predictions = np.zeros(len(X))
    for (name, model), weight in zip(models.items(), weights):
        pred = model.predict(X)
        predictions += weight * pred
    return predictions

# ì„±ëŠ¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì„¤ì • (RÂ² ê¸°ë°˜)
if len(trained_models) >= 3:
    ensemble_weights = [0.4, 0.35, 0.25]  # ìƒìœ„ 3ê°œ ëª¨ë¸
    ensemble_models = list(trained_models.items())[:3]
else:
    ensemble_weights = [1.0]  # ë‹¨ì¼ ëª¨ë¸
    ensemble_models = list(trained_models.items())[:1]

# ì•™ìƒë¸” ì˜ˆì¸¡
try:
    ensemble_pred = weighted_ensemble_predict(X_clean, dict(ensemble_models), ensemble_weights)
    ensemble_r2 = r2_score(y_clean, ensemble_pred)
    ensemble_rmse = np.sqrt(mean_squared_error(y_clean, ensemble_pred))
    
    print(f"ì•™ìƒë¸” RÂ²: {ensemble_r2:.4f}")
    print(f"ì•™ìƒë¸” RMSE: {ensemble_rmse:.4f}")
except Exception as e:
    print(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")

# 7) ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ê°œì„ ëœ ë²„ì „)
print("\n=== í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ===")

def objective_improved(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'max_depth': trial.suggest_int('max_depth', 2, 8),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])
    }
    
    model = RandomForestRegressor(**params, random_state=42)
    
    try:
        # êµì°¨ê²€ì¦ ìˆ˜í–‰
        cv = KFold(n_splits=3, shuffle=True, random_state=42)
        r2_scores = cross_val_score(model, X_clean, y_clean, cv=cv, scoring='r2')
        
        # RÂ²ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë³€ê²½
        return -r2_scores.mean()  # ìŒìˆ˜ë¡œ ë°˜í™˜í•˜ì—¬ ìµœì†Œí™” ë¬¸ì œë¡œ ë³€í™˜
    except:
        return 1000  # ì˜¤ë¥˜ ì‹œ í° ê°’ ë°˜í™˜

try:
    study_improved = create_study(direction='minimize')
    study_improved.optimize(objective_improved, n_trials=20)  # 50ì—ì„œ 20ìœ¼ë¡œ ì¤„ì„
    
    print(f"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {study_improved.best_params}")
    print(f"ìµœì  RÂ²: {-study_improved.best_value:.4f}")
    
    # ìµœì  ëª¨ë¸ë¡œ ìµœì¢… í‰ê°€
    best_model_improved = RandomForestRegressor(**study_improved.best_params, random_state=42)
    cv = KFold(n_splits=3, shuffle=True, random_state=42)
    r2_scores_final = cross_val_score(best_model_improved, X_clean, y_clean, cv=cv, scoring='r2')
    rmse_scores_final = np.sqrt(-cross_val_score(best_model_improved, X_clean, y_clean, cv=cv, scoring='neg_mean_squared_error'))
    
    print(f"\n=== ìµœì¢… ê²°ê³¼ ===")
    print(f"ê°œì„ ëœ ìµœì  RÂ²: {r2_scores_final.mean():.4f} Â± {r2_scores_final.std():.4f}")
    print(f"ê°œì„ ëœ ìµœì  RMSE: {rmse_scores_final.mean():.4f} Â± {rmse_scores_final.std():.4f}")
    
    # ê¸°ì¡´ ê²°ê³¼ì™€ ë¹„êµ
    print(f"\n=== ì„±ëŠ¥ ë¹„êµ ===")
    print(f"ê¸°ì¡´ RÂ²: 0.3429")
    print(f"ê°œì„ ëœ RÂ²: {r2_scores_final.mean():.4f}")
    print(f"ê°œì„ ë„: {(r2_scores_final.mean() - 0.3429) / 0.3429 * 100:.1f}%")
    
except Exception as e:
    print(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤íŒ¨: {str(e)}")
    print("ê¸°ë³¸ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ê¸°ë³¸ ëª¨ë¸ë¡œ ìµœì¢… í‰ê°€
    basic_model = RandomForestRegressor(random_state=42, n_estimators=100)
    cv = KFold(n_splits=3, shuffle=True, random_state=42)
    r2_scores_final = cross_val_score(basic_model, X_clean, y_clean, cv=cv, scoring='r2')
    
    print(f"\n=== ìµœì¢… ê²°ê³¼ (ê¸°ë³¸ ëª¨ë¸) ===")
    print(f"ê¸°ë³¸ ëª¨ë¸ RÂ²: {r2_scores_final.mean():.4f} Â± {r2_scores_final.std():.4f}")

# 8) ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
print("\n" + "="*60)
print("=== ìƒì„¸í•œ ì„±ëŠ¥ í‰ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ===")
print("="*60)

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
import os
results_dir = "model_results_ì§„ë£Œê³¼_ì „ë¬¸ì˜"
os.makedirs(f"{results_dir}/performance", exist_ok=True)
os.makedirs(f"{results_dir}/predictions", exist_ok=True)
os.makedirs(f"{results_dir}/models", exist_ok=True)

print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {results_dir}/")

# 8-1) ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
print("\n1/3: ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")

performance_results = {}
prediction_results = {}

# train/test ë¶„ë¦¬ (ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)

for name, model in trained_models.items():
    print(f"  - {name} ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    try:
        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        train_r2 = r2_score(y_train, y_pred_train)
        test_r2 = r2_score(y_test, y_pred_test)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        train_mae = mean_absolute_error(y_train, y_pred_train)
        test_mae = mean_absolute_error(y_test, y_pred_test)
        
        # ì„±ëŠ¥ ê²°ê³¼ ì €ì¥
        performance_results[name] = {
            'model_name': name,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'train_rmse': train_rmse,
            'test_rmse': test_rmse,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'overfitting_score': train_r2 - test_r2  # ê³¼ì í•© ì§€í‘œ
        }
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
        test_pred_df = X_test.copy()
        test_pred_df['y_actual'] = y_test.values
        test_pred_df['y_predicted'] = y_pred_test
        test_pred_df['prediction_error'] = y_test.values - y_pred_test
        test_pred_df['absolute_error'] = np.abs(y_test.values - y_pred_test)
        test_pred_df['model'] = name
        
        prediction_results[name] = test_pred_df
        
        print(f"    âœ… {name} - Test RÂ²: {test_r2:.4f}, Test RMSE: {test_rmse:.4f}")
        
    except Exception as e:
        print(f"    âŒ {name} í‰ê°€ ì‹¤íŒ¨: {str(e)}")
        continue

# 8-2) ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
print("\n2/3: ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")

if len(trained_models) >= 3:
    # ê°€ì¤‘ ì•™ìƒë¸” ëª¨ë¸
    ensemble_weights = [0.4, 0.35, 0.25]
    ensemble_models = list(trained_models.items())[:3]
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    y_pred_train_ensemble = np.zeros(len(X_train))
    y_pred_test_ensemble = np.zeros(len(X_test))
    
    for (name, model), weight in zip(ensemble_models, ensemble_weights):
        y_pred_train_ensemble += weight * model.predict(X_train)
        y_pred_test_ensemble += weight * model.predict(X_test)
    
    # ì•™ìƒë¸” ì„±ëŠ¥ ì§€í‘œ
    ensemble_train_r2 = r2_score(y_train, y_pred_train_ensemble)
    ensemble_test_r2 = r2_score(y_test, y_pred_test_ensemble)
    ensemble_train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_ensemble))
    ensemble_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_ensemble))
    ensemble_train_mae = mean_absolute_error(y_train, y_pred_train_ensemble)
    ensemble_test_mae = mean_absolute_error(y_test, y_pred_test_ensemble)
    
    # ì•™ìƒë¸” ì„±ëŠ¥ ê²°ê³¼ ì €ì¥
    performance_results['Weighted_Ensemble'] = {
        'model_name': 'Weighted_Ensemble',
        'train_r2': ensemble_train_r2,
        'test_r2': ensemble_test_r2,
        'train_rmse': ensemble_train_rmse,
        'test_rmse': ensemble_test_rmse,
        'train_mae': ensemble_train_mae,
        'test_mae': ensemble_test_mae,
        'overfitting_score': ensemble_train_r2 - ensemble_test_r2,
        'ensemble_weights': ensemble_weights,
        'ensemble_models': [name for name, _ in ensemble_models]
    }
    
    # ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    ensemble_pred_df = X_test.copy()
    ensemble_pred_df['y_actual'] = y_test.values
    ensemble_pred_df['y_predicted'] = y_pred_test_ensemble
    ensemble_pred_df['prediction_error'] = y_test.values - y_pred_test_ensemble
    ensemble_pred_df['absolute_error'] = np.abs(y_test.values - y_pred_test_ensemble)
    ensemble_pred_df['model'] = 'Weighted_Ensemble'
    
    prediction_results['Weighted_Ensemble'] = ensemble_pred_df
    
    print(f"    âœ… Weighted Ensemble - Test RÂ²: {ensemble_test_r2:.4f}, Test RMSE: {ensemble_test_rmse:.4f}")
    print(f"    ğŸ“Š ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {ensemble_weights}")
    print(f"    ğŸ”§ ì•™ìƒë¸” ëª¨ë¸: {[name for name, _ in ensemble_models]}")

# 8-3) ìµœì  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
print("\n3/3: ìµœì  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")

try:
    # ìµœì  ëª¨ë¸ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼)
    if 'study_improved' in locals():
        best_model = RandomForestRegressor(**study_improved.best_params, random_state=42)
        best_model.fit(X_train, y_train)
        
        y_pred_train_best = best_model.predict(X_train)
        y_pred_test_best = best_model.predict(X_test)
        
        best_train_r2 = r2_score(y_train, y_pred_train_best)
        best_test_r2 = r2_score(y_test, y_pred_test_best)
        best_train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train_best))
        best_test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test_best))
        best_train_mae = mean_absolute_error(y_train, y_pred_train_best)
        best_test_mae = mean_absolute_error(y_test, y_pred_test_best)
        
        # ìµœì  ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼ ì €ì¥
        performance_results['Optimized_RF'] = {
            'model_name': 'Optimized_RF',
            'train_r2': best_train_r2,
            'test_r2': best_test_r2,
            'train_rmse': best_train_rmse,
            'test_rmse': best_test_rmse,
            'train_mae': best_train_mae,
            'test_mae': best_test_mae,
            'overfitting_score': best_train_r2 - best_test_r2,
            'best_params': study_improved.best_params
        }
        
        # ìµœì  ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        best_pred_df = X_test.copy()
        best_pred_df['y_actual'] = y_test.values
        best_pred_df['y_predicted'] = y_pred_test_best
        best_pred_df['prediction_error'] = y_test.values - y_pred_test_best
        best_pred_df['absolute_error'] = np.abs(y_test.values - y_pred_test_best)
        best_pred_df['model'] = 'Optimized_RF'
        
        prediction_results['Optimized_RF'] = best_pred_df
        
        print(f"    âœ… Optimized RF - Test RÂ²: {best_test_r2:.4f}, Test RMSE: {best_test_rmse:.4f}")
        print(f"    ğŸ”§ ìµœì  íŒŒë¼ë¯¸í„°: {study_improved.best_params}")
    
except Exception as e:
    print(f"    âŒ ìµœì  ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {str(e)}")

# 9) ê²°ê³¼ ì €ì¥
print("\n" + "="*60)
print("=== ê²°ê³¼ ì €ì¥ ì‹œì‘ ===")
print("="*60)

# 9-1) ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥
print("1/4: ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥ ì¤‘...")

if performance_results:
    performance_df = pd.DataFrame(performance_results).T
    performance_df = performance_df.sort_values('test_r2', ascending=False)
    
    # CSV ì €ì¥
    performance_df.to_csv(f"{results_dir}/performance/model_performance_comparison.csv", encoding='utf-8-sig')
    
    # ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Test RÂ² ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ):")
    print(performance_df[['test_r2', 'test_rmse', 'test_mae', 'overfitting_score']].round(4))
    
    print(f"âœ… ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_dir}/performance/model_performance_comparison.csv")

# 9-2) ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
print("\n2/4: ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")

for name, pred_df in prediction_results.items():
    try:
        # CSV ì €ì¥
        pred_df.to_csv(f"{results_dir}/predictions/{name}_predictions.csv", encoding='utf-8-sig', index=False)
        print(f"  âœ… {name} ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {results_dir}/predictions/{name}_predictions.csv")
        
        # ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print(f"    ğŸ“ˆ {name} ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:")
        print(f"      - ì‹¤ì œê°’ í‰ê· : {pred_df['y_actual'].mean():.2f}")
        print(f"      - ì˜ˆì¸¡ê°’ í‰ê· : {pred_df['y_predicted'].mean():.2f}")
        print(f"      - í‰ê·  ì ˆëŒ€ ì˜¤ì°¨: {pred_df['absolute_error'].mean():.2f}")
        print(f"      - ìµœëŒ€ ì ˆëŒ€ ì˜¤ì°¨: {pred_df['absolute_error'].max():.2f}")
        
    except Exception as e:
        print(f"  âŒ {name} ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# 9-3) í†µí•© ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
print("\n3/4: í†µí•© ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì¤‘...")

try:
    # ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í†µí•©
    all_predictions = []
    
    for name, pred_df in prediction_results.items():
        # ê¸°ë³¸ ì •ë³´ë§Œ ì„ íƒ
        basic_cols = ['y_actual', 'y_predicted', 'prediction_error', 'absolute_error', 'model']
        feature_cols = [col for col in pred_df.columns if col not in basic_cols]
        
        # í”¼ì²˜ì™€ ì˜ˆì¸¡ ê²°ê³¼ë§Œ í¬í•¨
        result_df = pred_df[feature_cols + basic_cols].copy()
        all_predictions.append(result_df)
    
    if all_predictions:
        combined_predictions = pd.concat(all_predictions, ignore_index=True)
        combined_predictions.to_csv(f"{results_dir}/predictions/combined_predictions.csv", encoding='utf-8-sig', index=False)
        print(f"âœ… í†µí•© ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {results_dir}/predictions/combined_predictions.csv")
        print(f"   ğŸ“Š ì´ {len(combined_predictions)}ê°œ ì˜ˆì¸¡ ê²°ê³¼")
        
except Exception as e:
    print(f"âŒ í†µí•© ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# 9-4) ëª¨ë¸ ì €ì¥
print("\n4/4: ëª¨ë¸ ì €ì¥ ì¤‘...")

import joblib

for name, model in trained_models.items():
    try:
        model_path = f"{results_dir}/models/{name}_model.pkl"
        joblib.dump(model, model_path)
        print(f"  âœ… {name} ëª¨ë¸ ì €ì¥: {model_path}")
    except Exception as e:
        print(f"  âŒ {name} ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# ìµœì  ëª¨ë¸ë„ ì €ì¥
if 'best_model' in locals():
    try:
        best_model_path = f"{results_dir}/models/Optimized_RF_model.pkl"
        joblib.dump(best_model, best_model_path)
        print(f"  âœ… Optimized RF ëª¨ë¸ ì €ì¥: {best_model_path}")
    except Exception as e:
        print(f"  âŒ Optimized RF ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

print("\n" + "="*60)
print("ğŸ‰ ëª¨ë“  ë¶„ì„ ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print("="*60)
print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: {results_dir}/")
print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ: performance/model_performance_comparison.csv")
print("ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼: predictions/")
print("ğŸ’¾ ëª¨ë¸ íŒŒì¼: models/")
print("="*60)

# 10) ìµœì¢… ì„±ëŠ¥ ìš”ì•½
print("\nğŸ“ˆ ìµœì¢… ì„±ëŠ¥ ìš”ì•½:")
if performance_results:
    best_model_name = performance_df.index[0]
    best_performance = performance_df.loc[best_model_name]
    
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"   - Test RÂ²: {best_performance['test_r2']:.4f}")
    print(f"   - Test RMSE: {best_performance['test_rmse']:.4f}")
    print(f"   - Test MAE: {best_performance['test_mae']:.4f}")
    print(f"   - ê³¼ì í•© ì ìˆ˜: {best_performance['overfitting_score']:.4f}")

print("\n" + "="*60)